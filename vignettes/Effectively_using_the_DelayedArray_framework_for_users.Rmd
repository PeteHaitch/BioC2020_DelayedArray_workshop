---
title: "Effectively using the DelayedArray framework as a user to support the analysis of large datasets"
author: "Peter Hickey"
date: "Last modified: June 5, 2020; Compiled: `r format(Sys.time(), '%B %d, %Y')`"
bibliography: "`r system.file(package = 'DelayedArrayWorkshop', 'vignettes', 'ref.bib')`"
output:
  rmarkdown::html_document:
   highlight: pygments
   toc: true
   toc_depth: 3
   fig_width: 5
vignette: >
  %\VignetteIndexEntry{Effectively using the DelayedArray framework as a user to support the analysis of large datasets}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding[ut8]{inputenc}
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE)
```

# Effectively using the DelayedArray framework as a user to support the analysis of large datasets

BioC 2019: 24-27 June

## Workshop description

This workshop gives an introductory overview of the DelayedArray framework, which can be used by Bioconductor packages to support the analysis of large datasets.
A *DelayedArray* is like an ordinary array in R, but allows for the data to be in-memory^[Including space-efficient formats like sparse arrays and run length encoded arrays], on-disk in a file, or even hosted on a remote server.
Participants will learn where they might encounter a *DelayedArray* in the wild while using Bioconductor and help them understand the fundamental concepts underlying the framework.
This workshop will be a mixture of lecture with example code and discussion.
Examples will mostly be drawn from the analysis of single-cell RNA-sequencing data.

### Instructor

- Peter Hickey (hickey@wehi.edu.au)

### Pre-requisites

- Basic knowledge of R syntax.
- Familiarity with common operations on matrices in R, such as `colSums()` and `colMeans()`.
- Some familiarity with S4 objects may be helpful but is not required.
- Some familiarity with single-cell RNA-sequencing may be helpful but is not required.

### Workshop Participation

Students will be able to run code interactively during the workshop.
There will be opportunities throughout the workshop for questions and discussion.

### _R_ / _Bioconductor_ packages used

- `r BiocStyle::Biocpkg("DelayedArray")`
- `r BiocStyle::Biocpkg("HDF5Array")`
- `r BiocStyle::Biocpkg("DelayedMatrixStats")`
- `r BiocStyle::Biocpkg("TENxBrainData")`
- `r BiocStyle::Biocpkg("TENxPBMCData")`
- `r BiocStyle::Biocpkg("BiocSingular")`
- `r BiocStyle::Biocpkg("scater")`
- `r BiocStyle::Biocpkg("scran")`

### Time outline

| Activity                                        | Time |
|-------------------------------------------------|------|
| Introductory material                           | 10m  |
| First contact                                   | 40m  |
| Package ecosystem                               | 10m  |
| Real world encounters analysing scRNA-seq data  | 25m  |
| Workflow tips for DelayedArray-backed analyses  | 15m  |

### Workshop goals and objectives

#### Learning goals

- Learn of existing packages and functions that *use* the DelayedArray framework.
- Develop a high-level understanding of classes and packages that *implement* the DelayedArray framework.
- Become familiar with the fundamental concepts of delayed operations, block-processing, and realization.
- Reason about potential bottlenecks in algorithms operating on *DelayedArray* objects.

#### Learning objectives

- Identify when an object is a *DelayedArray* or one of its derivatives.
- Be able to recognise when it is useful to use a *DelayedArray* instead of an ordinary array or other array-like data structure.
- Learn how to load and save a DelayedArray-backed object.
- Learn how the 'block size' and 'chunking' of the dataset affect performance when operating on *DelayedArray* objects.
- Take away some miscellaneous tips and tricks I've learnt over the years when working with DelayedArray-backed objects.

## Introductory material

Data from a high-throughput biological assay, such as single-cell RNA-sequencing (scRNA-seq), will often be summarised as a matrix of counts, where rows correspond to features and columns to samples^[Higher-dimensional arrays may be appropriate for some types of assays.].
Within **Bioconductor**, the *SummarizedExperiment* class is the recommended container for such data, offering a rich interface that tightly links assay measurements to data on the features and the samples.

The *SummarizedExperiment* class is used to store rectangular arrays of experimental results (_assays_).
Here, each *assay* is drawn as a matrix but higher-dimensional arrays are also supported.

```{r include = FALSE}
# download current version of SE diagram
se_path <- file.path(tempdir(), "SE.svg")
download.file(
  "https://docs.google.com/feeds/download/drawings/Export?id=1kiC8Qlo1mhSnLDqkGiRNPSo6GWn3C2duBszCFbJCB-g&exportFormat=svg", 
  se_path)
```

`r knitr::include_graphics(se_path)`

Traditionally, the assay data are stored in-memory as an ordinary *array* object^[In R, a *matrix* is just a 2-dimensional *array*]. 
Storing the data in-memory becomes a real pain with the ever-growing size of 'omics datasets.
It is now not uncommon to collect $10,000-100,000,000$ measurements on $100 - 1,000,000$ samples, which would occupy $10-1,000$ gigabytes (Gb) if stored in-memory as ordinary R arrays.

The DelayedArray framework offers a solution to this problem.
Wrapping an array-like object (typically an on-disk object) in a *DelayedArray* object allows one to perform common array operations on it without loading the object in memory.
In order to reduce memory usage and optimize performance, operations on the object are either delayed or executed using a block processing mechanism.

### Projects enabled by DelayedArray

The DelayedArray framework enables the analysis of datasets that are too large to be stored or processed in-memory.
This has become particularly relevant with the advent of large single-cell RNA-sequencing (scRNA-seq) studies containing tens of thousands to millions of cells.

In my own research I have made extensive use of the DelayedArray framework when analysing whole genome bisulfite sequencing (WGBS) datasets.
To give a recent example, we profiled 45 human brain samples using WGBS to measure DNA methylation [@rizzardi2019neuronal].
For most tissues, we focus on so-called so-called CpG methylation, which requires analysing matrices with roughly 20 million rows (CpG loci) and 45 columns (samples).
This sized data is challenging, but well within the realms of high performance computing available at a modern research institute.
However, the brain also has extensive non-CpG methylation and there are an order of magnitude more non-CpG loci.
This necessitated extensive re-factoring of our software tools and we successfully adopted the DelayedArray framework to enable this research.

In this workshop we will see how the analysis of large scRNA-seq datasets is enabled by the DelayedArray framework.

## First contact

The heart of the DelayedArray framework is implemented in the `r BiocStyle::Biocpkg("DelayedArray")` package, which we now load.

```{r}
library(DelayedArray)
```

We will begin with an example using some scRNA-seq data on 1.3 million brain cells from embryonic mice, generated by 10X Genomics.
These data are available in the `r BiocStyle::Biocpkg("TENxBrainData")` Bioconductor package.

```{r}
# NOTE: The TENxBrainData package loads and attaches the HDF5Array package, 
#       amongst others.
library(TENxBrainData)

# NOTE: This will download the data and may take a little while on the first 
#       run. The result will be cached, however, so subsequent runs avoid 
#       re-downloading the data.
tenx <- TENxBrainData()
```

Let's take a look at the `tenx` object.

```{r}
tenx
```

The data are stored in a *SingleCellExperiment* object, an extension of the *SummarizedExperiment* class.
With data from 1.3 million cells, this is roughly 100,000-times more samples
than a typical bulk RNA-seq dataset and would occupy 146 GB in-memory if stored as an ordinary *matrix*.

We might expect that interacting with an object containing this many samples would feel sluggish, but this is not the case.
For example, we can efficiently subset the object.

```{r}
# Take a random sample of 1000 genes and 200000 samples
tenx[sample(nrow(tenx), 1000), sample(ncol(tenx), 200000)]
```

Now, this efficiency could all be down to some trickery through the outer *SingleCellExperiment* class.
To make things clearer, we'll extract the `counts` assay.

```{r}
# NOTE: We'll discuss the use of `withDimnames = FALSE` later in the workshop.
tenx_counts <- assay(tenx, "counts", withDimnames = FALSE)
```

Now, let's do something that would ordinarily be a terrible idea, and something that's frustrated me way too many times: let's "accidentally" print out the entire counts matrix.

```{r}
tenx_counts
```

Hallelujah!
Unlike what you may have experienced when printing out a large matrix, this didn't overwhelm the screen with thousands of lines of output nor did it cause the R session to hang indefinitely.
In fact, this gives us a rather pretty printing of the counts matrix^[You may have seen similar pretty printing with other Bioconductor objects such as *GRanges* and *DataFrame* or with the *data.table* and *tibble* extensions to the *data.frame*. I can't say enough how much I appreciate these thoughtful touches when doing interactive data analysis.].
No need for panicked mashing of `Ctrl-c` or `Esc`. 

We can now clearly see that `tenx_counts` is no ordinary *matrix*.
In fact, it is an *HDF5Matrix*, which is a type of *DelayedArray*^[As with a 2-dimensional *array* in base R being commonly known as a *matrix*, a 2-dimensional *DelayedArray* is also known as a *DelayedMatrix* and a 2-dimensional *HDF5Array* is also known as a *HDF5Matrix*.].

```{r}
class(tenx_counts)
is(tenx_counts, "DelayedArray")
```

The data contained in an *HDF5Matrix* is actually stored on disk in a [Hierarchical Data Format (**HDF5**)](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) file.
Consequently, the `tenx_counts` object takes up very little space in memory.

```{r}
print(object.size(tenx_counts), units = "auto")
```

We can learn more about the internals of the `tenx_counts` object using the `seed()` function^[We will revisit the `chunkdim` component of the *HDF5Matrix* object later in the workshop in [Chunking]].

```{r}
seed(tenx_counts)
```

### Three examples of computing on a DelayedArray

We will now play around with computing on the counts matrix.
To make things slightly easier, we will first subset the data to 1000 samples.

```{r}
tenx_counts_subset <- tenx_counts[, 1:1000]
```

#### Library sizes

Firstly, let's compute the library sizes for this subset of samples.
We can do this using `colSums()`.

```{r}
lib_sizes <- colSums(tenx_counts_subset)
summary(lib_sizes)
```

#### Proportion of cells with non-zero expression for each gene

Secondly, suppose we want to know for each gene the proportion of cells with non-zero expression.
We can do this using `rowSums()` in conjunction with some standard R commands (logical comparisons and division).

```{r}
prop_non_zero <- rowSums(tenx_counts_subset > 0) /  ncol(tenx_counts_subset)
summary(prop_non_zero)
```

#### Median expression of each gene

Finally, suppose we want to know the median expression of each gene.
Here, we will quantify expression as counts per million (CPM) using library size normalization.

```{r}
cpm <- t(t(1e6 * tenx_counts_subset) / lib_sizes)
cpm
```

We can then compute the median expression of each gene using `DelayedMatrixStats::rowMedians()`.

```{r}
library(DelayedMatrixStats)
median_expression <- rowMedians(cpm)
summary(median_expression)
```

#### Summary

These 3 examples highlight the power of the DelayedArray framework.
Recall that the data in these examples live on disk in an HDF5 file, yet we interacted with `tenx_counts_subset` and computed on it much as we would if the data were in-memory as an ordinary matrix.
Also note that all 3 examples returned ordinary R vectors.

```{r}
class(lib_sizes)
class(prop_non_zero)
class(cpm) # TODO: Not an ordinary vector.
```

To do so, we made (implicit) use of the three fundamental concepts of the DelayedArray framework:

1. Delayed operations
2. Block-processing
3. Realization

We'll now discuss each of these in turn.

### Delayed operations

Taking a careful look at `tenx_counts_subset`, we see that it is a *DelayedMatrix* rather than an *HDF5Matrix*.

```{r}
tenx_counts_subset
```

The subsetting operation has 'degraded' the `tenx_counts_subset` object to a *DelayedMatrix*.

```{r}
is(tenx_counts_subset, "HDF5Matrix")
is(tenx_counts_subset, "DelayedMatrix")
```

The `showtree()` function can help us see what changed when we subsetted the data.

```{r}
showtree(tenx_counts)
showtree(tenx_counts_subset)
```

The subsetting operation has been registered in what is termed a 'delayed operation'.
Registering a delayed operation does not modify the underlying data.
Instead, the operation is recorded and only performed when the *DelayedArray* object is 'realized'.
Realization of a *DelayedArray* triggers the execution of the delayed operations carried by the object and returns the result as an ordinary *array*.

This allows us to chain together multiple operations and only perform them as required.
Here is a contrived example.

```{r}
# Add 1 to every element (a delayed op).
x <- tenx_counts_subset + 1L
showtree(x)

# Compute log of every element (another delayed op).
lx <- log(x)
showtree(lx)

# Transpose the result (another delayed op).
tlx <- t(lx)
showtree(tlx)

# Realize a subset of the data as an ordinary matrix.
as.array(tlx[1:5, 1:10])
```

Many common operations can be registered as delayed operations.
Here are some examples^[The technical names of each type of delayed operation are not important.].
Notice that in each case the result is 'degraded' to a *DelayedMatrix*^[The [No-op] example is the obvious exception].

#### DelayedSubset 

```{r}
val <- tenx_counts[, 1:100]
val
showtree(val)
```

#### DelayedAperm

```{r}
val <- t(tenx_counts)
val
showtree(val)
```

#### DelayedUnaryIsoOp

```{r}
val <- tenx_counts + 1L
val
showtree(val)
val <- tenx_counts + 1:2
val
showtree(val)
```

#### DelayedSubassign

```{r}
# NOTE: Be careful with delayed subassignment. You can end up with objects that 
#       are surprisingly large in-memory.
tmp <- tenx_counts
tmp[, 1] <- 100
tmp
showtree(tmp)
```

#### DelayedDimnames

```{r}
tmp <- tenx_counts
rownames(tmp) <- paste0("R", seq_len(nrow(tmp)))
tmp
showtree(tmp)
```

#### DelayedNaryIsoOp

```{r}
val <- tenx_counts + tenx_counts
val
showtree(val)
```

#### DelayedAbind

```{r}
val <- cbind(tenx_counts, tenx_counts)
val
showtree(val)
```

#### No-op

The DelayedArray framework is smart enough to recognise that some combinations of operations are 'no-ops'.

```{r}
val <- t(t(tenx_counts))
val
showtree(val)
```

But it can be fooled.

```{r}
# NOTE: This is a no-op but DelayedArray doesn't recognise it as one.
val <- tenx_counts + 0L
val
showtree(val)

# NOTE: This also a no-op but DelayedArray doesn't recognise it as one.
val <- cbind(tenx_counts) # TODO: No longer true
val
showtree(val)
```

### Block-processing

In [Library sizes], we needed to compute the column sums of `tenx_counts_subset`, whose data live on disk in an HDF5 file.
One way of achieving this would be to load the entire dataset into memory as an ordinary matrix and then run `base::colSums()`^[Here, I use the 'namespaced function' notation for didactic purposes to distinguish `base::colSums()` from the S4 generic `colSums()` and its associated methods. When writing code, however, it is generally not necessary to use the `package::function()` notation, except for all the times that it is ...].

```{r}
lib_sizes_in_mem <- colSums(as.array(tenx_counts_subset))
# Check we get the same result as before.
identical(lib_sizes_in_mem, lib_sizes)
```

Here the data at `r print(object.size(as.array(tenx_counts_subset)), units = "auto")` are small enough to load into memory, but what if that's not the case^[For example, recall that the `tenx_counts` data would occupy > 146 GB in memory.]?
One way you might think to do this is to loop over the columns of the matrix, load that column into memory, and compute it's sum^[In the following code example we could replace `as.array(colSums(tenx_counts_subset[, j, drop = FALSE]))` with `sum(tenx_counts_subset[, j, drop = TRUE])` or, more simply, `sum(tenx_counts_subset[, j])`. This is because subsetting a *DelayedArray* returns an ordinary vector when `drop = TRUE` and the result has only one dimension; see 'Subsetting' in `help("DelayedArray", package = "DelayedArray")`.]

```{r}
lib_sizes_loop_over_cols <- vector("numeric", ncol(tenx_counts_subset))
for (j in seq_len(ncol(tenx_counts_subset))) {
  # A simple progress report.
  if (j %% 100 == 0) message("Processed ", j, "/", ncol(tenx_counts_subset))
  lib_sizes_loop_over_cols[j] <- colSums(
    as.array(tenx_counts_subset[, j, drop = FALSE]))
}
# Check we get the same result as before.
identical(lib_sizes_loop_over_cols, lib_sizes)
```

But what if you can't load even a single column into memory^[This may seem artificial, but suppose we were dealing with a very 'tall' matrix.]?
We might loop over the columns of the matrix, partition each column, load each partition, compute its sum, and then compute the sum of the partition sums.

```{r}
# NOTE: Here we will partition each column into two equal-sized subsets.
lib_sizes_loop_over_cols_subset <- vector("numeric", ncol(tenx_counts_subset))
tmp_colsums <- vector("numeric", 2)
nr <- nrow(tenx_counts_subset)

for (j in seq_len(ncol(tenx_counts_subset))) {
  # A simple progress report.
  if (j %% 100 == 0) message("Processed ", j, "/", ncol(tenx_counts_subset))
  for (i in 1:2) {
    i1 <- (i - 1) * nr / 2 + 1
    i2 <- i * nr / 2
    tmp_colsums[i] <- colSums(
      as.array(tenx_counts_subset[seq(i1, i2), j, drop = FALSE]))
  }
  lib_sizes_loop_over_cols_subset[j] <- sum(tmp_colsums)
}
# Check we get the same result as before.
identical(lib_sizes_loop_over_cols_subset, lib_sizes)
```

Hopefully you can now begin to see the general pattern, a strategy which the `r BiocStyle::Biocpkg("DelayedArray")` package calls 'block-processing':

1. Load a 'block' of the data into memory.
2. Compute a summary statistic.
3. Combine the block-level statistics in an appropriate way to get the final result.

#### Block-processing illustrated

Some examples of block-processing are illustrated in the following figures:

`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide1.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide2.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide3.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide4.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide5.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide6.png"))`
`r knitr::include_graphics(file.path(system.file(package = 'DelayedArrayWorkshop', 'vignettes'), "Slide7.png"))`

#### Block-processed column sums

When we run `colSums(tenx_counts_subset`) we are using a block-processed version of column sums, specifically the `colSums,DelayedMatrix-method` implemented in the `r BiocStyle::Biocpkg("DelayedArray")` package.
We can see this more clearly by turning on verbose progress reporting from the `r BiocStyle::Biocpkg("DelayedArray")` package.

```{r}
# Turn on progress reports for DelayedArray's block-processing routines.
DelayedArray:::set_verbose_block_processing(TRUE)
head(colSums(tenx_counts_subset))
```

#### More examples of block-processed operations in `r BiocStyle::Biocpkg("DelayedArray")`

Some of the most useful functions in the `r BiocStyle::Biocpkg("DelayedArray")` package implement common operations on a *DelayedMatrix* using block-processing.
These include the following row and column summarization methods:

- `rowSums()`
- `colSums()`
- `rowMeans()`
- `colMeans()`
- `rowMaxs()`
- `colMaxs()`
- `rowMins()`
- `colMins()`
- `rowRanges()`
- `colRanges()`

Two useful but lesser known functions use block-processing to compute column/row sums of a *DelayedMatrix* based on a grouping variable:

- `rowsum()`
- `colsum()`

```{r}
# Compute separate library sizes for mitochondrial and non-mitochondrial genes.
is_mito <- grepl("^mt-", rowData(tenx)$Symbol)
summary(is_mito)
lib_sizes_grouped <- rowsum(tenx_counts_subset, group = is_mito)
head(t(lib_sizes_grouped))
```

Finally, matrix multiplication is implemented as a block-processed operation.

```{r}
# This is mathematically equivalent to rowSums(tenx_counts_subset).
# TODO: Is this still using block-processing?
tenx_counts_subset %*% matrix(1, nrow = ncol(tenx_counts_subset))
```

#### DelayedMatrixStats

We've already seen the `r BiocStyle::Biocpkg("DelayedMatrixStats")` package in action back when computing the [Median expression of each gene].
`r BiocStyle::Biocpkg("DelayedMatrixStats")` is a port of the `r BiocStyle::CRANpkg("matrixStats")` package's API for use with *DelayedMatrix* objects.
It provides [more than 70 functions](https://github.com/PeteHaitch/DelayedMatrixStats#api-coverage) that apply to rows and columns of *DelayedMatrix* objects

#### General block-processing

As we have seen, many common block-processing operations on a *DelayedMatrix* have already been implemented in `r BiocStyle::Biocpkg("DelayedArray")` or are provided by `r BiocStyle::Biocpkg("DelayedMatrixStats")`.
Nonetheless, there may be times you need to implement your own algorithm using block-processing.
The documentation on this topic is a little sparse, but some details can be found in `help("block_processing", "DelayedArray")` and `help("ArrayGrid", "DelayedArray")` or by reading the source code of the aforementioned packages.
Briefly, to perform block-processing requires that you:

1. Set up an *ArrayGrid* over your *DelayedArray*. This specifies the 'block' structure that will be traversed when processing the *DelayedArray*.
2. Iterate over the *DelayedArray* via the *ArrayGrid*
    a. Read each block of data into memory as an ordinary array using `read_block()`.
    b. Compute the statistic for that block
3. Appropriately combine the block-level statistics to get your final result.

The `blockApply()` and `blockReduce()` functions can help facilitate steps 1-3 , even incorporating parallelization via the `r BiocStyle::Biocpkg("BiocParallel")` package.

### Realization

To *realize* a *DelayedArray* object is to trigger execution of the delayed operations carried by the object and return the result as an ordinary array,
One way to achieve this is to call `as.array()` on it.

```{r}
tenx_counts_subset_realized <- as.array(tenx_counts_subset)
tenx_counts_subset_realized[1:10, 1:10]
```

However, this realizes the full object at once in memory which could require too much memory if the object is big^[In the above example it's safe because `tenx_counts_subset_realized` only requires around 100 Mb of memory.].

A large *DelayedArray* object is preferably realized on disk, which is most commonly an HDF5 file.

#### Realizing to an HDF5 file

Realizing to an HDF5 file requires that the `r BiocStyle::Biocpkg("HDF5Array")` package is installed^[The `r BiocStyle::Biocpkg("TENxBrainData")` package depends upon `r BiocStyle::Biocpkg("HDF5Array")`, so it is already installed. If you've been following this workflow then you've also already attached it to the search path by running `library(TENxBrainData)`.]

```{r}
tenx_counts_subset_hdf5 <- writeHDF5Array(tenx_counts_subset)
tenx_counts_subset_hdf5

# Alternatively, we could 'coerce' the result to be an HDF5Array.
as(tenx_counts_subset, "HDF5Array")
```

Notice that this process of realization used block-processing, which avoids loading the entire dataset entire memory.
Also notice that the result of realization is here returned as an *HDF5Matrix*, which no longer carries around the delayed operations.

```{r}
class(tenx_counts_subset)
class(tenx_counts_subset_hdf5)

showtree(tenx_counts_subset)
showtree(tenx_counts_subset_hdf5)
```

Used like this, `writeHDF5Array()` and `as(..., "HDF5Array")` will write their results to a file in *HDF5 dump directory*, a dumping ground for automatically created HDF5 datasets.
We can see a log of the operations that have written to the HDF5 dump directory using `showHDF5DumpLog()`.

```{r}
showHDF5DumpLog()
```

Often, however, we will want full control of where and how the data are written to the HDF5 file^[We'll discuss why you might want this in the second half of the workshop.] and the `writeHDF5Array()` function gives you full control over this and more.

```{r}
# Write the data to a user-specified HDF5 file using maximum compression and 
# 'chunking' along the columns.
my_hdf5_file <- tempfile(fileext = ".h5")
tenx_counts_subset_my_file_hdf5 <- writeHDF5Array(
  tenx_counts_subset,
  filepath = my_hdf5_file,
  chunkdim = c(nrow(tenx_counts_subset), 1),
  level = 9)

# Compare `tenx_counts_subset_hdf5` to `tenx_counts_subset_my_file_hdf5`
seed(tenx_counts_subset_hdf5)
seed(tenx_counts_subset_my_file_hdf5)
```

#### Realization backends

We've now seen that we can realize to an HDF5 file.
This is called as the HDF5Array 'realization backend' and is implemented in the `r BiocStyle::Biocpkg("HDF5Array")` package.
There are a few other realization backends to be aware of.

```{r}
supportedRealizationBackends()
```

There is also the `NULL` backend, which means the data are realized in memory as an ordinary *array* and then wrapped in a *DelayedArray*.
This is the default realization backend upon loading/attaching the `r BiocStyle::Biocpkg("DelayedArray")` package.

```{r}
getRealizationBackend()
```

The default realization backend can be altered with `setRealizationBackend()`.

```{r}
setRealizationBackend("HDF5Array")
getRealizationBackend()
setRealizationBackend(NULL)
getRealizationBackend()
```

It can be important to know what your current realization backend is because it will be used implicitly by some functions.
For example, matrix multiplication that involves a *DelayedMatrix* uses the current realization backend.

```{r}
setRealizationBackend("HDF5Array")
tenx_counts_subset %*% matrix(1, nrow = ncol(tenx_counts_subset))

setRealizationBackend(NULL)
tenx_counts_subset %*% matrix(1, nrow = ncol(tenx_counts_subset))
```

#### The `realize()` function

We've seen that we can realize to the HDF5Array backend using `writeHDF5Array(tenx_counts_subset)` and `as(tenx_counts_subset, "HDF5Array")`.
A third way of realizing a *DelayedArray* to an HDF5 file is with the `realize()` function.

```{r}
realize(tenx_counts_subset, BACKEND = "HDF5Array")
```

So why might you use `realize()` instead of these other options?
Because it allows us to easily switch out the realization backend and will defer to the current realization backend if none is supplied.

```{r}
# Realize to the current realization backend.
getRealizationBackend()
realize(tenx_counts_subset)

# Realize as an RleArray.
setRealizationBackend("RleArray")
tenx_counts_subset_rlearray <- realize(tenx_counts_subset)
tenx_counts_subset_rlearray

# Switch back to the default backend.
setRealizationBackend(NULL)
```

This is probably most useful when writing package code so that you can allow the user control over the realization backend.

## Package ecosystem

The DelayedArray framework is, unsurprisingly, implemented in the `r BiocStyle::Biocpkg("DelayedArray")` package.
However, there are several other key packages that are an important part of the broader 'ecosystem'.
More importantly, as a user of Bioconductor software, it is increasingly likely that you will encounter *DelayedArray* objects during a data analysis, especially if you are analysing single-cell data^[In fact, if you use any package that makes use of the *SummarizedExperiment* class, then you will almost certainly load the `r BiocStyle::Biocpkg("DelayedArray")` package during the course of your analysis, whether you know it or not! This is because `r BiocStyle::Biocpkg("SummarizedExperiment")` depends upon `r BiocStyle::Biocpkg("DelayedArray")`.].
The following table lists packages that depend upon the `r BiocStyle::Biocpkg("DelayedArray")` package.

```{r}
dep_tbl <- BiocPkgTools::buildPkgDependencyDataFrame()
da_dep_tbl <- dep_tbl[dep_tbl$dependency == "DelayedArray", 
                      c("Package", "edgetype")]
da_dep_tbl <- da_dep_tbl[with(da_dep_tbl, order(edgetype, Package)), ]
colnames(da_dep_tbl) <- c("Package", "Dependency Type")
rmarkdown::paged_table(da_dep_tbl)
```

We will briefly highlight some of the key packages in this table, broadly categorising these as 'user-focused'/'user-facing' or 'developer-focused' packages and those that span the spectrum.

### Packages that both users and developers should probably know about

#### `r BiocStyle::Biocpkg("DelayedArray")`

Well, duh.
Implements the *DelayedArray* and *RleArray* classes, along with all the fundamentals the enable the delayed operations, block processing, and realization that underpin the DelayedArray framework.

#### `r BiocStyle::Biocpkg("HDF5Array")`

Implements the *HDF5Array* and *TENxMatrix* classes, two convenient and memory-efficient array-like containers for on-disk representation of HDF5 datasets. 
*HDF5Array* is for datasets that use the conventional (i.e. dense) HDF5 representation.
*TENxMatrix* is for datasets that use the HDF5-based sparse matrix representation from 10x Genomics.

#### `r BiocStyle::Biocpkg("VCFArray")` and `r BiocStyle::Biocpkg("GDSArray")`

Implements the *VCFArray* and *GDSArray* classes, types of *DelayedArray*, to represent VCF files and GDS-files in an array-like representation.
VCF and GDS files are widely used to represent genotyping or sequence data.

#### `r BiocStyle::Biocpkg("rhdf5client")` and `r BiocStyle::Biocpkg("restfulSE")`

Provide functions and classes to interface with remote data stores by operating on *SummarizedExperiment*-like objects.
These data are HDF5 files living on a remote server running `h5serv`, a REST-based service for HDF5 data.

### User-focused/user-facing packages

These are the packages that as a user you might directly load/attach with `library()` as part of a data analysis.
Alternatively, these may be loaded/attached as a dependency^[As listed in the `Depends` field of the package `DESCRIPTION` file.] of another package you load/attach as part of an analysis.

#### `r BiocStyle::Biocpkg("DropletUtils")`

Provides a number of utility functions for handling single-cell (RNA-seq) data from droplet technologies such as 10X Genomics.
This includes `read10xCounts()` for data loading from the count matrices produced by 10x Genomics' **CellRanger** software, which may be stored in an HDF5 file.
To do this, it makes use of the *TENxMatrix* class.

#### `r BiocStyle::Biocpkg("LoomExperiment")`

Provides a means to convert from 'loom' files to standard Bioconductor classes and back again.
The [Loom file format](http://linnarssonlab.org/loompy/index.html) uses HDF5 to store experimental data and is used by some tools and labs producing data using single-cell assays.
This includes the `import()` function for data loading from loom files into an *HDF5Matrix*.

#### `r BiocStyle::Biocpkg("scater")`

A collection of tools for doing various analyses of single-cell RNA-seq gene expression data, with a focus on quality control and visualization.
These methods work on *DelayedMatrix* objects as well as ordinary *matrix* objects and some sparse matrix objects from the `r BiocStyle::CRANpkg("Matrix")` package.

#### `r BiocStyle::Biocpkg("batchelor")`

Implements a variety of methods for batch correction of single-cell (RNA sequencing) data, such as`multiBatchPCA()` and `fastMNN()`.
These methods work on *DelayedMatrix* objects as well as ordinary *matrix* objects and some sparse matrix objects from the `r BiocStyle::CRANpkg("Matrix")` package.

#### `r BiocStyle::Biocpkg("BiocSingular")`

Implements exact and approximate methods for singular value decomposition and principal components analysis using a framework that allows them to be easily switched within Bioconductor packages or workflows.
These methods work on *DelayedMatrix* objects as well as ordinary *matrix* objects and some sparse matrix objects from the `r BiocStyle::CRANpkg("Matrix")` package.

#### `r BiocStyle::Biocpkg("mbkmeans")`

Implements the mini-batch k-means algorithm for large datasets, including support for on-disk data representation (i.e. *HDF5Matrix* objects).

#### `r BiocStyle::Biocpkg("bsseq")`

A collection of tools for analyzing and visualizing bisulfite sequencing data.
This was one of the first packages to make use of the DelayedArray framework and it supports these throughout the package.
This was needed in order to store and analyse large non-CpG methylation datasets  (> 300 million loci, hundreds of samples) using HDF5 files.
Disclaimer: I did this re-write of `r BiocStyle::Biocpkg("bsseq")` and learnt a lot along the way.

#### `r BiocStyle::Biocpkg("minfi")`

Tools to analyze & visualize Illumina Infinium methylation arrays.
This doesn't have the same level of support for *DelayedMatrix* objects as `r BiocStyle::Biocpkg("bsseq")`, but perhaps one day.
This is needed in order to store and analyse large methylation datasets (> 850,000 loci, tens of thousands of) using HDF5 files.
Disclaimer: This was the second package, after `r BiocStyle::Biocpkg("bsseq")`, I started to re-write to support the DelayedArray framework.
Here, it is rather more difficult because it is a 'widely' used package and has code from lots of different authors with different styles.

#### `r BiocStyle::Biocpkg("DelayedMatrixStats")`

A port of the `r BiocStyle::CRANpkg("matrixStats")` API for use with *DelayedMatrix* objects.
High-performing functions operating on rows and columns of *DelayedMatrix* objects, e.g. `col` / `rowMedians()`, `col` / `rowRanks()`, and `col` / `rowSds()`.
Functions optimized per data type and for subsetted calculations such that both memory usage and processing time is minimized.
Disclaimer: I wrote this one, too.

### Developer-focused packages

#### `r BiocStyle::Biocpkg("beachmat")`

Provides a consistent C++ class interface for reading from and writing data to a variety of commonly used matrix types.
Ordinary matrices and several sparse/dense `r BiocStyle::CRANpkg("Matrix")` classes are directly supported, third-party S4 classes may be supported by external linkage (such as the *HDF5Matrix* class), while all other matrices are handled by DelayedArray block processing.

## Real world encounter with DelayedArray analysing scRNA-seq data

**TODO:** Remove this section? Didn't get through it last time and it overlaps with OSCA and other workshops.

We will perform a brief analysis of a peripheral blood mononuclear cell (PBMC) scRNA-seq dataset from 10X Genomics [@zheng2017massively] to illustrate a real world analysis that makes use of the DelayedArray framework^[This is adapted from the `r BiocStyle::Biocpkg("simpleSingleCell")` workflow.].

This dataset contains more than 4000 cells, which isn't that large in the grand scheme of things, but does allow us to demonstrate some important concepts in a workshop format.
It has been pre-packaged and is available from the `r BiocStyle::Biocpkg("TENxPBMCData")` package as a *SingleCellExperiment* object with a *HDF5Matrix* in the counts assay.
In order to efficiently analyse this data, we would like to use software that supports the *HDF5Matrix* class via the DelayedArray framework.
Thanks to the tireless efforts of Aaron Lun, this is true of many of the key Bioconductor packages for analysing scRNA-seq data: `r BiocStyle::Biocpkg("DropletUtils")`, `r BiocStyle::Biocpkg("scater")`, `r BiocStyle::Biocpkg("scran")`, and `r BiocStyle::Biocpkg("BiocSingular")`.

#### Loading the data

A typical analysis of a 10x Genomics dataset would start by using the `DropletUtils::read10xCounts()` function to load the output of 10x Genomics' CellRanger software into R.
One of the CellRanger outputs is an HDF5 file, which `DropletUtils::read10xCounts()` can read and return as a *TENxMatrix*, an HDF5-backed *DelayedMatrix*.
For this workshop, we'll use one we prepared earlier and simply load the 'filtered' dataset from the `r BiocStyle::Biocpkg("TENxPBMCData")` package which stores the count data in an *HDF5Matrix*.

```{r, eval = FALSE}
library(TENxPBMCData)
# NOTE: This will download the data and may take a little while on the first 
#       run. The result will be cached, however, so subsequent runs avoid 
#       re-downloading the data.
sce <- TENxPBMCData("pbmc4k")
sce
# NOTE: The counts data are stored in an HDF5Matrix.
class(assay(sce, withDimnames = FALSE))
```

We'll also switch to use the gene symbols provided by 10x Genomics rather than referring to genes by their Ensembl IDs.

```{r, eval = FALSE}
library(scater)
rownames(sce) <- uniquifyFeatureNames(
  rowData(sce)$ENSEMBL_ID,
  rowData(sce)$Symbol_TENx)
```

#### Quality control on the cells

After loading the data into R, we compute some quality control metrics of the data using `scater::addPerCellQC()`.
Under the hood, this involves computing column sums, comparisons of values, and subsetting of the HDF5-backed count matrix, all seamlessly handled by this function.

```{r, eval = FALSE}
# NOTE: Need to know which genes are mitochondrial to perform QC.
library(EnsDb.Hsapiens.v86)
location <- mapIds(
  EnsDb.Hsapiens.v86,
  keys = rowData(sce)$ENSEMBL_ID, 
  column = "SEQNAME", 
  keytype = "GENEID")
is_mito <- location == "MT"

library(scater)
sce <- addPerCellQC(sce, feature_controls = list(Mito = which(is_mito)))
multiplot(
  plotColData(sce, "sum") + 
    ylab("Log-total UMI count") + 
    scale_y_log10(),
  plotColData(sce, "detected") + 
    ylab("Log-total number of expressed features") + 
    scale_y_log10(),
  plotColData(sce, "subsets_Mito_percent") +
    ylab("Proportion of reads in mitochondrial genes"),
  cols = 2)

# We'll discard cells with large mitochondrial proportions, using it as a proxy
# for cell damage.
high_mito <- isOutlier(sce$pct_counts_Mito, nmads = 3, type = "higher")
sce <- sce[, !high_mito]
```

### Examining gene expression

We can make a plot of the distribution of the most highly expressed genes using `scater::plotHighestExprs()`.
Under the hood, this involves computing row and column sums of the HDF5-backed count matrix.
Again, this is seamlessly handled by this function.

```{r, eval = FALSE}
plotHighestExprs(sce)
```

### Normalization

Let's move onto something a little more computationally challenging.
Proper normalization is essential for all analyses of gene expression data.
We apply the deconvolution method of Lun, Bach, and Marioni (2016) to compute size factors for all cells.

For highly heterogeneous datasets, like this sample of PBMCs, it is advisable to perform a rough clustering of the cells to better satisfy the assumptions of this normalization method.
Namely, we want to avoid normalizing together cells with a large number of differentially expressed genes between them.

We will use the `scran::quickCluster()` function to perform a clustering based on the principal component scores generated from the log-expression matrix.
This principal component analysis (PCA) in turn uses an approximate singular value decomposition (SVD) with the augmented implicitly restarted Lanczos bidiagonalization algorithm (**irlba**).
If that all sounds rather complicated, then don't worry: that's the point of this example!
We are able to apply these cutting edge techniques to our HDF5-backed data^[For the sake of time, we're going to cheat by loading the counts matrix into memory as a sparse matrix and computing the clustering using the in-memory data. But this functionality does work, albeit more slowly, for the HDF5-backed data.].

```{r, eval = FALSE}
library(scran)
set.seed(1000)
clusters <- quickCluster(
  # NOTE: We're gonna cheat here to save time in the workshop.
  #       We'll load the counts into memory as a sparse matrix and compute the 
  #       clustering on that.
  x = as(counts(sce, withDimnames = FALSE), "dgCMatrix"),
  use.ranks = FALSE, 
  BSPARAM = BiocSingular::IrlbaParam())
table(clusters)
```

Following the clustering, we apply the deconvolution method to compute size factors for all cells [@lun2016pooling].

```{r, eval = FALSE}
sce <- computeSumFactors(sce, min.mean = 0.1, cluster = clusters)
```

Finally, we compute normalized log-expression values from the count data using the size factors we just computed.
Normalized expression values are computed by dividing the counts for each cell by the size factor for that cell.

```{r, eval = FALSE}
sce <- normalize(sce)
# NOTE: Normalized counts are available in the 'logcounts' assay.
logcounts(sce)
```

This final step is near instantaneous because it is a delayed operation on the counts matrix.

```{r, eval = FALSE}
showtree(logcounts(sce, withDimnames = FALSE))
# The counts and logcounts assays share the same seed.
seed(counts(sce))
seed(logcounts(sce))
```

### Dimensionality reduction

Skipping over some details, we use functions from the `r BiocStyle::Biocpkg("scran")` package to decompose the variation in the log-expression matrix to a technical and biological component.

```{r, eval = FALSE}
tech_trend <- makeTechTrend(x = sce)
fit <- trendVar(sce, use.spikes = FALSE, loess.args = list(span = 0.05))
fit$trend <- tech_trend
dec <- decomposeVar(fit = fit)
```

We can then perform a principal component analysis (PCA) of the log-expression matrix to create a representation of the data using fewer dimensions.
In doing so, we also denoise the log-expression data by removing PCs corresponding to technical noise.

In the interests of time, we are again going to cheat by loading the log-expression matrix into memory as a sparse matrix before doing the PCA.
However, the same function works on the HDF5-backed data, albeit more slowly than when applied to an in-memory version of the data.

```{r, eval = FALSE}
set.seed(1000)
pcs <- denoisePCA(
  # NOTE: We're gonna cheat here to save time. 
  #       We'll load the log-expression matrix into memory as a sparse matrix 
  #       and compute the PCA on that.
  #       To run on the HDF5-backed data, simply use `x = logcounts(sce)` 
  #       (which will return the PCs an ordinary matrix) or `x = sce` (which 
  #       will return a SingleCellExperiment that is a modified version of 
  #       `sce` containing the PCs as an ordinary matrix as the "PCA" entry in 
  #       the reducedDims slot).
  x = as(logcounts(sce, withDimnames = FALSE), "dgCMatrix"),
  technical = tech_trend,
  BSPARAM = BiocSingular::IrlbaParam())
# NOTE: Store the PCs in the SingleCellExperiment.
reducedDim(sce, "PCA") <- pcs
```

### Clustering and visualization

Our *SingleCellExperiment* object now contains:

1. The counts matrix: stored on disk in an HDF5 file and wrapped in a *DelayedMatrix*.
2. The normalized log-expression matrix: represented by a *DelayedMatrix* as delayed operations on top of the counts matrix.
3. The principal components (PCs): stored in-memory as an ordinary matrix

The key at this point is that PCs are stored in-memory.
Much of the downstream analysis makes use of this representation rather than the counts or log-expression matrices.
In particular, we demonstrate that we can perform a clustering and $t$-SNE representation of the data.

```{r, eval = FALSE}
# Clustering
snn_gr <- buildSNNGraph(sce, use.dimred = "PCA")
clusters <- igraph::cluster_walktrap(snn_gr)
sce$Cluster <- factor(clusters$membership)
table(sce$Cluster)

# t-SNE
set.seed(1000)
sce <- runTSNE(sce, use_dimred = "PCA", perplexity = 30)

# Visualize clustering on t-SNE plot.
plotTSNE(sce, colour_by = "Cluster")
```

### Concluding remarks

This analysis illustrates that we can analyse scRNA-seq data that is stored on disk using tools that leverage the DelayedArray framework.
More importantly, the same functions and workflow apply regardless of how the data are stored.

The biggest discrepancy in performance is when performing the principal component analysis of the data.
However, by trading off additional computational time for reduced memory requirements, this is fully achievable and the resulting PCs can be saved for downstream analysis.
This puts the DelayedArray-backed analysis on par with a fully in-memory-based analysis and with the advantage of a reduced in-memory footprint.

## Workflow tips for DelayedArray-backed analyses

We'll conclude with some miscellaneous tips I've collected over the past few years of using DelayedArray-backed workflows.

### Saving and loading HDF5-backed SummarizedExperiment objects

```{r}
# TODO: Better/simpler example dataset
library(TENxPBMCData)
library(scran)
library(scater)
# NOTE: This will download the data and may take a little while on the first 
#       run. The result will be cached, however, so subsequent runs avoid 
#       re-downloading the data.
sce <- TENxPBMCData("pbmc4k")
sce <- computeSumFactors(sce, min.mean = 0.1)
sce <- logNormCounts(sce)
```

#### Short version

Use `HDF5Array::saveHDF5SummarizedExperiment()` and `HDF5Array::loadHDF5SummarizedExperiment()` rather than `saveRDS()` and `readRDS()` or `save()` and `load()`.
Calling `HDF5Array::saveHDF5SummarizedExperiment()` will realize any delayed operations prior to saving the assay data in an HDF5 file, as illustrated below.

```{r}
dir <- file.path(tempdir(), "my_h5_se")
saveHDF5SummarizedExperiment(sce, dir, verbose = TRUE)

# NOTE: There are no delayed operations on the saved version.
saved_sce <- loadHDF5SummarizedExperiment(dir)
showtree(logcounts(sce, withDimnames = FALSE))
showtree(logcounts(saved_sce, withDimnames = FALSE))
```

#### Long version

A HDF5-backed *SummarizedExperiment*, like the 10x PBMC dataset we analysed in [Real world encounter with DelayedArray analysing scRNA-seq data], is a light-weight shell (the *SummarizedExperiment*) around a large disk-backed data matrix (the *HDF5Matrix*).
The following explanation comes from `help("saveHDF5SummarizedExperiment", "HDF5Array")`:

Roughly speaking, `saveRDS()` only serializes the part of an object that resides in memory^[The reality is a little bit more nuanced, but discussing the full details is not important here, and would only distract us].
For most objects in R, that's the whole object, so `saveRDS()` does the job.

However some objects are pointing to on-disk data.
For example:

- A *TxDb* object from the `r BiocStyle::Biocpkg("GenomicFeatures")` points to an SQLite db
- An *HDF5Array* object points to a dataset in an HDF5 file
- A *SummarizedExperiment* derivative can have one or more of its assays that point to datasets (one per assay) in an HDF5 file. 

These objects have 2 parts: one part is in memory, and one part is on disk.
The 1st part is sometimes called the object shell and is generally thin (i.e. it has a small memory footprint).
The 2nd part is the data and is typically big.
The object shell and data are linked together via some kind of pointer stored in the shell (e.g. an SQLite connection, or a path to a file, etc.).
Note that this is a one way link in the sense that the object shell "knows" where to find the on-disk data but the on-disk data knows nothing about the object shell (and is completely agnostic about what kind of object shell could be pointing to it).
Furthermore, at any given time on a given system, there could be more than one object shell pointing to the same on-disk data.
These object shells could exist in the same R session or in sessions in other languages (e.g. Python).
These various sessions could be run by the same or by different users.

Using `saveRDS()` on such object will only serialize the shell part so will produce a small `.rds` file that contains the serialized object shell but not the object data.

This is problematic because:

1. If you later unserialize the object (with `readRDS()`) on the same system where you originally serialized it, it is possible that you will get back an object that is fully functional and semantically equivalent to the original object. But here is the catch: this will be the case **ONLY** if the data is still at the original location and has not been modified (i.e. nobody wrote or altered the data in the SQLite db or HDF5 file in the mean time), and if the serialization/unserialization cycle didn't break the link between the object shell and the data (this serialization/unserialization cycle is known to break open SQLite connections).
2. After serialization the object shell and data are stored in separate files (in the new `.rds` file for the shell, still in the original SQLite or HDF5 file for the data), typically in very different places on the file system. But these 2 files are not relocatable, that is, moving or copying them to another system or sending them to collaborators will typically break the link between them. Concretely this means that the object obtained by using `readRDS()` on the destination system will be broken.

`saveHDF5SummarizedExperiment()` addresses these issues by saving the object shell and assay data in a folder that is relocatable.

Note that it only works on *SummarizedExperiment* derivatives.
What it does exactly is:

1. Write all the assay data to an HDF5 file
2. Serialize the object shell, which in this case is everything in the object that is not the assay data.

The 2 files (HDF5 and `.rds`) are written to the directory specified by the user.
The resulting directory contains a full representation of the object and is relocatable, that is, it can be moved or copied to another place on the system, or to another system (possibly after making a tarball of it), where `loadHDF5SummarizedExperiment()` can then be used to load the object back in R.

### Avoid subsetting too much and random access patterns

Reordering/subsetting the data may degrade the performance of even seemingly simple operations.
This is especially true of HDF5-backed data, where performance is best when reading contiguous chunks of data and worst when having to read data with a random access pattern.

```{r}
x <- counts(sce, withDimnames = FALSE)
y <- x[sample(nrow(x)), sample(ncol(x))]
system.time(colSums(x))
system.time(colSums(y))
```

### Process, save, repeat

When analysing large datasets, a workflow that is broken up into stages and saves the intermediate outputs can be help preserve ones sanity.
This is true regardless of whether the DelayedArray framework is used: it sucks having to repeat some long pre-processing computation in order to make a quick plot.
But it is especially true for DelayedArray-backed analyses where the accumulation of delayed operations will eventually lead to degraded performance.
This means using `saveHDF5SummarizedExperiment()`/`loadHDF5SummarizedExperiment()`, or `realize()`-ing the result if there is sufficient memory available, following time consuming processing of the object.

### Pragmatism rules

We've seen some examples of pragmatism in this workshop: just load the data into memory, compute the thing you need, and move on.
For the workshop, I used it to 'cheat' in order to speed things up, but it is frequently a valid strategy when analysing data!

For example, a normalized scRNA-seq dataset carries around two matrices: the raw counts and the normalized expression values.
You might have enough RAM to load one of these a time but not both at once.
With a HDF5-backed *SingleCellExperiment* you can easily just load into memory the matrix you actually need at a given step in the analysis.

Another example, with WGBS data you carry around 3 very large matrices.
But to make a plot of methylation values along a gene promoter, a common requirement, you only need to load in a small 'slice' of one of these matrices.
With a HDF5-backed *BSseq*^[A *SummarizedExperiment* derivative] object you can quickly do this.

This brings us to a perhaps underappreciated advantage of using HDF5-backed *SummarizedExperiment* derivatives, namely that loading the saved data with `HDF5Array::loadHDF5SummarizedExperiment()` is really fast.
I made extensive use of this when processing large WGBS datasets as I could quickly load the *BSseq* object to compute summaries of the sample metadata (stored in the `colData` of the *BSseq* object), a process that used to take tens of minutes to hours because the 3 large matrices also had to be loaded into memory.
This has been so useful to me that I now keep even 'small' WGBS datasets as HDF5-backed *BSseq* objects.

This pragmatism has served me well.
Often times I find myself starting with a very large dataset, do my initial processing using the disk-backed data, and run `saveHDF5SummarizedExperiment()` to produce a 'pristine' object.
I can then move it to a large-memory machine, do my big computation (e.g., PCA), save the result, and **move on with my life**.

### Avoid 'degrading' to a *DelayedArray*

The DelayedArray framework is implemented using the S4 object oriented system.
This can be used to write methods that are optimized for a particular backend.
For example, we might write a `colMaxs()` method that is optimized for the *TENxMatrix* class by exploiting the sparse storage mode of the underlying data.
In order for `colMaxs()` to 'know' that it can use this optimized method, however, it needs for the data to be supplied as a *TENxMatrix* instance.

Unfortunately, it is very easy to 'degrade' a specialised *DelayedArray* derivative to a *DelayedArray*.

```{r}
# Let's create a TENxMatrix
tenx_matrix <- as(counts(sce), "TENxMatrix")
class(tenx_matrix)

# NOTE: All these degrade the result to a DelayedMatrix.
class(tenx_matrix[, 1:10])
val <- tenx_matrix
dimnames(val) <- list(paste0("R", seq_len(nrow(val))), NULL)
class(val)
```

A common scenario where this degrading may occur is when extracting the data from a *SummarizedExperiment*.

```{r}
tmp <- SingleCellExperiment(
  list(counts = tenx_matrix),
  colData = DataFrame(row.names = paste0("S", seq_len(ncol(tenx_matrix)))))
class(assay(tmp))
```

What's happened here?
By default, `assay(tmp)` calls `assay(tmp, withDimnames = TRUE)` which has the effect of copying the dimnames from the *SummarizedExperiment* and adding them to the return assay data.
As we saw above, setting the dimnames on a *TENxMatrix* (or other *DelayedMatrix* derivative) will degrade it to a *DelayedMatrix*.
Consequently, running `colMaxs(assay(tmp))` will not call our (hypothetical) optimized method for *TENxMatrix* objects and will instead defer to the slower, more general block-processing method that is implemented for *DelayedMatrix* objects.

To avoid this 'degrading upon assay extraction', we can should set `withDimnames = FALSE`.

```{r}
class(assay(tmp, withDimnames = FALSE))
```

More generally, you may need to avoid degrading a *DelayedArray* derivative to a *DelayedArray* in order to use backend-optimized methods.

### Block size 

The maximum size of a block used when performing block-processing is given by `getAutoBlockSize()`.
By default, this is set to `r as.integer(getAutoBlockSize())` meaning each block can use up to `r as.integer(getAutoBlockSize())` / `1e6` = `r getAutoBlockSize() / 1e6` Mb of data.

Using fewer, larger blocks generally means faster performance (at the cost of higher peak memory usage), so we may wish to increase this on machines with sufficient memory by using `setAutoBlockSize()`.

```{r}
system.time(colSums(counts(sce, withDimnames = FALSE)))

# NOTE: Increasing the block size 10-fold.
setAutoBlockSize(getAutoBlockSize() * 10)
system.time(colSums(counts(sce, withDimnames = FALSE)))

# NOTE: Revert to default block size.
setAutoBlockSize(getAutoBlockSize() / 10)
```

### Chunking

Data stored in an HDF5 file are usually 'chunked' into sub-matrices (for matrix data) or hyper-cubes (for arrays of higher dimension)^[Chunking also applies to other storage formats, such as *RleArray* objects, but we'll focus on the HDF5 format.].
For example, we could choose to chunk an $R \times C$ matrix by column, by row, or into $r \times c$ sub-matrices ($r \leq R, c \leq C$).
This may remind you of the choice of block dimensions used in block-processing.
The difference is this: 

> **Block dimensions dictate how the data are accessed, chunk dimensions dictate how the data are stored**.

In general, you want your data to be chunked in a manner that supports the type of access patterns you will be making.
For example, if you know you only need to access data by column then chunk the data by column.
Of course, you often either don't know in advance what access patterns you need or you need both row and column access.
In that case, chunking into sub-matrices offers the best tradeoff.

We'll demonstrate how chunking can affect performance by comparing computing the column sums of a column-chunked and row-chunked dataset.

```{r}
x <- matrix(sample(1e8), ncol = 1e2, nrow = 1e6)
x_col <- writeHDF5Array(x, chunkdim = c(nrow(x), 1))
x_row <- writeHDF5Array(x, chunkdim = c(1, ncol(x)))

system.time(colSums(x_col))
system.time(colSums(x_row))
```

Chunking can be controlled by the `chunkdim` argument when writing data to disk using the `writeHDF5Array()` or `saveHDF5SummarizedExperiment()` functions.

### Parallelization strategies

Several of the routines we used in today's workshop have 'native' parallelization via the `r BiocStyle::Biocpkg("BiocParallel")` package.
Look out for the `BPPARAM` argument in function documentation^[Unfortunately, simply typing the function name at the command prompt or using `args()` may not be sufficient to identify these functions. E.g., `args(denoisePCA)` doesn't include `BPPARAM` but `help("denoisePCA", "scran")` tells you this is an option (Footnote to the footnote: this is because `denoicePCA()` is an S4 generic with methods defined only for the first argument, `x`. The methods include the `BPPARAM` argument and are documented as such.)].

Fair warning: parallelization is tricky and performance may not match expectations.

### What's in my HDF5 file?

I do this quite a bit: I've got an HDF5 file but I don't know (or have forgotten) what's in it.
`rhdf5::h5ls()` to the rescue.

```{r}
rhdf5::h5ls(path(counts(sce)))
```

## Concluding remarks

We've now spent nearly 2 hours learning about the DelayedArray framework.
Now comes the kicker: 

> **Don't use a *DelayedArray* if you can avoid it!**

I write that as someone who has a professional love for the DelayedArray framework.
I've spent many, many hours developing software that supports and extends it, leveraging it to analyse large WGBS and scRNA-seq datasets, and writing and presenting talks and workshops to teach it.
But the simple fact remains:

> **If you can load your data into memory and still compute on it then you're always going to have a better time doing it that way.**

Analyses will be faster, simpler, and you will have more options available to you.
But when this isn't an option then the DelayedArray framework is a powerful set of packages to help you get your work done.
I find it pretty remarkable that a first-class single-cell analysis workflow can so seamlessly support the use of in-memory and disk-backed data.

A huge amount of gratitude is owed to Hervé Pagès, the developer of the `r BiocStyle::Biocpkg("DelayedArray")` package and the broader framework, and Aaron Lun, the main developer many of the key packages for analysing scRNA-seq data and the `r BiocStyle::Biocpkg("simpleSingleCell")` workflow that I cribbed from in [Real world encounter with DelayedArray analysing scRNA-seq data].
Thank you!

## Session info

This document was prepared using the following software:

```{r}
sessionInfo()
```

## References
