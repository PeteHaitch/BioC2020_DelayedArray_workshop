---
title: "Effectively using the DelayedArray framework as a user to support the analysis of large datasets"
subtitle: "Presented at BioC 2020 (27-31 July)"
author: "Peter Hickey"
date: "Last modified: July 14, 2020; Compiled: `r format(Sys.time(), '%B %d, %Y')`"
bibliography: "`r system.file(package = 'DelayedArrayWorkshop', 'vignettes', 'ref.bib')`"
output:
  rmarkdown::html_document:
   highlight: pygments
   toc: true
   toc_depth: 3
   fig_width: 5
vignette: >
  %\VignetteIndexEntry{Effectively using the DelayedArray framework as a user to support the analysis of large datasets}
  %\VignetteEncoding[ut8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
resource_files:
  - images/SummarizedExperiment.svg
  - images/*.png
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE)
```

## Workshop description

This workshop gives an introductory overview of the DelayedArray framework, which can be used by R / Bioconductor packages to support the analysis of large array-like datasets.
A *DelayedArray* is like an ordinary array in R, but allows for the data to be in-memory, on-disk in a file, or even hosted on a remote server.

Workshop participants will learn where they might encounter a *DelayedArray* in the wild while using Bioconductor and help them understand the fundamental concepts underlying the framework.
This workshop will feature [introductory material](https://docs.google.com/presentation/d/1_v4IuKLFs781pp7x0BwUxv7mwCfPLBTWu320g3lQmME/edit?usp=sharing), 'live' coding, and Q&A.

### Instructor

- [Peter Hickey](https://peterhickey.org/) (hickey@wehi.edu.au)

### Pre-requisites

- Basic knowledge of R syntax.
- Familiarity with common operations on matrices in R, such as `colSums()` and `colMeans()`.
- Some familiarity with S4 objects may be helpful but is not required.

### Workshop Participation

Students will be able to run code examples from the workshop material.
There will be a Q&A session in the second half of the workshop.

### *R* / *Bioconductor* packages used

These packages are the focus of this workshop:

- `r BiocStyle::Biocpkg("DelayedArray")`
- `r BiocStyle::Biocpkg("HDF5Array")`
- `r BiocStyle::Biocpkg("DelayedMatrixStats")`

Please see the workshop `DESCRIPTION` for a full list of dependencies.

### Time outline

| Activity                                       | Time   |
|------------------------------------------------|--------|
| Introductory material                          | 5 min  |
| First contact                                  | 20 min |
| Workflow tips for DelayedArray-backed analyses | 15 min |
| Q&A                                            | 15 min |

### Workshop goals and objectives

#### Learning goals

- Learn of existing packages and functions that *use* the DelayedArray framework.
- Develop a high-level understanding of classes and packages that *implement* the DelayedArray framework.
- Become familiar with the fundamental concepts of delayed operations, block-processing, and realization.
- Reason about potential bottlenecks, and how to avoid or reduce these, in algorithms operating on *DelayedArray* objects.

#### Learning objectives

- Identify when an object is a *DelayedArray* or one of its derivatives.
- Be able to recognise when it is useful to use a *DelayedArray* instead of an ordinary array or other array-like data structure.
- Learn how to load and save a DelayedArray-backed object.
- Learn how the 'block size' and 'chunking' of the dataset affect performance when operating on *DelayedArray* objects.
- Take away some miscellaneous tips and tricks I've learnt over the years when working with DelayedArray-backed objects.

## Introductory material

**TODO**: Make this material into slides?

Data from a high-throughput biological assay, such as single-cell RNA-sequencing (scRNA-seq), will often be summarised as a matrix of counts, where rows correspond to features and columns to samples[^1].
Within **Bioconductor**, the *SummarizedExperiment* class is the recommended container for such data, offering a rich interface that tightly links assay measurements to data on the features and the samples.

[^1]: Higher-dimensional arrays may be appropriate for some types of assays.

The *SummarizedExperiment* class is used to store rectangular arrays of experimental results (*assays*). Here, each *assay* is drawn as a matrix but higher-dimensional arrays are also supported.

`r knitr::include_graphics("images/SummarizedExperiment.svg")`

Traditionally, the assay data are stored in-memory as an ordinary *array* object[^2]. 
Storing the data in-memory becomes a real pain with the ever-growing size of 'omics datasets. 
It is now not uncommon to collect $10,000-100,000,000$ measurements on $100 - 1,000,000$ samples, which would occupy $10-1,000$ gigabytes (Gb) if stored in-memory as ordinary R arrays.

[^2]: In R, a *matrix* is just a 2-dimensional *array*

The DelayedArray framework offers a solution to this problem. 
Wrapping an array-like object (typically an on-disk object) in a *DelayedArray* object allows one to perform common array operations on it without loading the object in memory. 
In order to reduce memory usage and optimize performance, operations on the object are either delayed or executed using a block processing mechanism.

### Projects enabled by DelayedArray

The DelayedArray framework enables the analysis of datasets that are too large to be stored or processed in-memory.
This has become particularly relevant with the advent of large single-cell RNA-sequencing (scRNA-seq) studies containing tens of thousands to millions of cells.

In my own research I have made extensive use of the DelayedArray framework when analysing whole genome bisulfite sequencing (WGBS) datasets.
To give a recent example, we profiled 45 human brain samples using WGBS to measure DNA methylation [@rizzardi2019neuronal].
For most tissues, we focus on so-called so-called CpG methylation, which requires analysing matrices with roughly 20 million rows (CpG loci) and 45 columns (samples).
This sized data is challenging, but well within the realms of high performance computing available at a modern research institute.
However, the brain also has extensive non-CpG methylation and there are an order of magnitude more non-CpG loci.
This necessitated extensive re-factoring of our software tools and we successfully adopted the DelayedArray framework to enable this research.

## First contact

The heart of the DelayedArray framework is implemented in the `r BiocStyle::Biocpkg("DelayedArray")` package, which we now load and attach.

```{r}
library(DelayedArray)
```

We'll also load and attach the `r BiocStyle::Biocpkg("HDF5Array")` package, which extends the DelayedArray framework to support on-disk HDF5 files.

```{r}
library(HDF5Array)
```

We will begin with an example using some scRNA-seq data on 1.3 million brain cells from embryonic mice, generated by 10X Genomics.
This dataset is available from `r BiocStyle::Biocpkg("ExperimentHub")`^[This dataset is also available in the `r BiocStyle::Biocpkg("TENxBrainData")` Bioconductor package.].

```{r}
library(ExperimentHub)
hub <- ExperimentHub()

# Query ExperimentHub to find the relevant resource.
# This dataset is available in two formats: a 'dense matrix' format and a
# 'HDF5-based 10X Genomics' format. We'll use the 'dense matrix' version for 
# this workshop.
query(hub, "TENxBrainData")
# Load the relevant resource.
# This will download the data and may take a little while on the first run. 
# The result will be cached, however, so subsequent runs avoid re-downloading 
# the data.
fname <- hub[["EH1040"]]

# The structure of this HDF5 file can be seen using the h5ls() command
# from the rhdf5 package:
library(rhdf5)
h5ls(fname)

# The 1.3 Million Brain Cell Dataset is represented by the "counts" group. 
# We point the HDF5Array() constructor to this group to create a HDF5Matrix 
# object representing the dataset:
tenx <- HDF5Array(filepath = fname, name = "counts")

# The data contain counts on nearly 28,000 gene for more than 
# 1.3 million cells.
dim(tenx)
```

With data from 1.3 million cells, this is roughly 100,000-times more samples than a typical bulk RNA-seq dataset and would require over 140 GB of RAM to hold as a matrix and around 30 GB as a sparse matrix.

With so much data, we might expect that it would feel sluggish to interact with this object, but this is not the case.
For example, let's do something that would ordinarily be a terrible idea, and something that's frustrated me way too many times: let's 'accidentally' print out the entire counts matrix.

```{r}
tenx
```

Hallelujah!
Unlike what you may have experienced when printing out a large matrix, this didn't overwhelm the screen with thousands of lines of output nor did it cause the R session to hang indefinitely.
In fact, this gives us a rather pretty printing of the counts matrix[^3].
No need for panicked mashing of `Ctrl-c` or `Esc`.

[^3]: You may have seen similar pretty printing with other Bioconductor objects such as *GRanges* and *DataFrame* or with the *data.table* and *tibble* extensions to the *data.frame*. I can't say enough how much I appreciate these thoughtful touches when doing interactive data analysis.

We can now clearly see that `tenx` is no ordinary *matrix*.
In fact, it is an *HDF5Matrix*, which is a type of *DelayedArray*[^4].

[^4]: As with a 2-dimensional *array* in base R being commonly known as a *matrix*, a 2-dimensional *DelayedArray* is also known as a *DelayedMatrix* and a 2-dimensional *HDF5Array* is also known as a *HDF5Matrix*.

```{r}
class(tenx)
is(tenx, "DelayedArray")
```

The data contained in an *HDF5Matrix* is actually stored on disk in a [Hierarchical Data Format (**HDF5**)](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) file.
Consequently, the `tenx` object takes up relatively little space in memory^[Most of the object's size is due to the dimnames being stored in memory.].

```{r}
print(object.size(tenx), units = "auto")
```

We can learn more about the internals of the `tenx` object using the `seed()` function.

```{r}
seed(tenx)
```

### Three examples of computing on a DelayedArray

We will now play around with computing on the counts matrix.
To make things slightly easier, we will first subset the data to 1000 samples.

```{r}
tenx_subset <- tenx[, 1:1000]
```

#### Library sizes

Firstly, let's compute the library sizes for this subset of samples.
We can do this using `colSums()`.

```{r}
lib_sizes <- colSums(tenx_subset)
summary(lib_sizes)
```

#### Proportion of cells with non-zero expression for each gene

Secondly, suppose we want to know for each gene the proportion of cells with non-zero expression.
We can do this using `rowSums()` in conjunction with some standard R commands (logical comparisons and division).

```{r}
prop_non_zero <- rowSums(tenx_subset > 0) /  ncol(tenx_subset)
summary(prop_non_zero)
```

#### Median expression of each gene

Finally, suppose we want to know the median expression of each gene.
Here, we will quantify expression as counts per million (CPM) using library size normalization.

```{r}
cpm <- t(t(1e6 * tenx_subset) / lib_sizes)
cpm
```

We can then compute the median expression of each gene using `DelayedMatrixStats::rowMedians()`.

```{r}
library(DelayedMatrixStats)
median_expression <- rowMedians(cpm)
summary(median_expression)
```

#### Summary

These 3 examples highlight the power of the DelayedArray framework.
Recall that the data in these examples live on disk in an HDF5 file, yet we interacted with `tenx_subset` and computed on it much as we would if the data were in-memory as an ordinary matrix.
Also note that all 3 examples returned ordinary R vectors.

```{r}
class(lib_sizes)
class(prop_non_zero)
class(median_expression)
```

To do so, we made (implicit) use of the three fundamental concepts of the DelayedArray framework:

1. Delayed operations
2. Block-processing
3. Realization

We'll now discuss each of these in turn.

### Delayed operations

Taking a careful look at `tenx_subset`, we see that it is a *DelayedMatrix* rather than an *HDF5Matrix*.

```{r}
tenx_subset
```

The subsetting operation has 'degraded' the `tenx_subset` object to a *DelayedMatrix*.

```{r}
is(tenx_subset, "HDF5Matrix")
is(tenx_subset, "DelayedMatrix")
```

The `showtree()` function can help us see what changed when we subsetted the data.

```{r}
showtree(tenx)
showtree(tenx_subset)
```

The subsetting operation has been registered in what is termed a 'delayed operation'.
Registering a delayed operation does not modify the underlying data.
Instead, the operation is recorded and only performed when the *DelayedArray* object is 'realized'.
Realization of a *DelayedArray* triggers the execution of the delayed operations carried by the object and returns the result as an ordinary *array*.

This allows us to chain together multiple operations and only perform them as required.
Here is a contrived example.

```{r}
# Add 1 to every element (a delayed op).
x <- tenx_subset + 1L
showtree(x)

# Compute log of every element (another delayed op).
# TODO: Demostrate that this preserves sparsity?
lx <- log(x)
showtree(lx)

# Transpose the result (another delayed op).
tlx <- t(lx)
showtree(tlx)

# Realize a subset of the data as an ordinary matrix.
as.array(tlx[1:5, 1:10])
```

**TODO:** Note that delayed ops are sparsity preserving.

Many common operations can be registered as delayed operations.
Here are some examples[^6].
Notice that in each case the result is 'degraded' to a *DelayedMatrix*[^7].

[^6]: The technical names of each type of delayed operation are not important.

[^7]: The [No-op] example is the obvious exception

#### DelayedSubset

```{r}
val <- tenx[, 1:100]
val
showtree(val)
```

#### DelayedAperm

```{r}
val <- t(tenx)
val
showtree(val)
```

#### DelayedUnaryIsoOp

```{r}
val <- tenx + 1L
val
showtree(val)
val <- tenx + 1:2
val
showtree(val)
```

#### DelayedSubassign

```{r}
tmp <- tenx
tmp[, 1] <- 100
tmp
showtree(tmp)
```

**WARNING**: Be careful with delayed subassignment because you can end up with objects that are surprisingly large in-memory.
This is because the subassigned values are kept in-memory until the data are *realized*.

#### DelayedDimnames

```{r}
tmp <- tenx
rownames(tmp) <- paste0("R", seq_len(nrow(tmp)))
tmp
showtree(tmp)
```

#### DelayedNaryIsoOp

```{r}
val <- tenx + tenx
val
showtree(val)
```

#### DelayedAbind

```{r}
val <- cbind(tenx, tenx)
val
showtree(val)
```

#### No-op

The DelayedArray framework is smart enough to recognise that some combinations of operations are 'no-ops'.

```{r}
val <- t(t(tenx))
val
showtree(val)
```

But it can be fooled.

```{r}
# This is a no-op but DelayedArray doesn't recognise it as one.
val <- tenx + 0L
val
showtree(val)
```

### Block-processing

In [Library sizes], we needed to compute the column sums of `tenx_subset`, whose data live on disk in an HDF5 file.
One way of achieving this would be to load the entire dataset into memory as an ordinary matrix and then run `base::colSums()`[^8].

[^8]: Here, I use the 'namespaced function' notation for didactic purposes to distinguish `base::colSums()` from the S4 generic `colSums()` and its associated methods. When writing code, however, it is generally not necessary to use the `package::function()` notation, except for all the times that it is ...

```{r}
lib_sizes_in_mem <- colSums(as.array(tenx_subset))
# Check we get the same result as before.
identical(lib_sizes_in_mem, lib_sizes)
```

The `tenx_subset` data are small enough to load into memory, but what if that's not the case[^9]?
One way you might think to do this is to loop over the columns of the matrix, load that column into memory, and compute it's sum[^10]

[^9]: Recall, For example, that the `tenx` data would occupy more than 140 GB in memory.

[^10]: In the following code example we could replace `as.array(colSums(tenx_subset[, j, drop = FALSE]))` with `sum(tenx_subset[, j, drop = TRUE])` or, more simply, `sum(tenx_subset[, j])`. This is because subsetting a *DelayedArray* returns an ordinary vector when `drop = TRUE` and the result has only one dimension; see 'Subsetting' in `help("DelayedArray", package = "DelayedArray")`.

```{r}
lib_sizes_loop_over_cols <- vector("numeric", ncol(tenx_subset))
for (j in seq_len(ncol(tenx_subset))) {
  # A simple progress report.
  if (j %% 100 == 0) message("Processed ", j, "/", ncol(tenx_subset))
  lib_sizes_loop_over_cols[j] <- colSums(
    as.array(tenx_subset[, j, drop = FALSE]))
}
# Check we get the same result as before.
# (We'll ignore the names of the result)
all.equal(lib_sizes_loop_over_cols, lib_sizes, 
          check.attributes = FALSE)
```

But what if you can't load even a single column into memory[^11]?
We might loop over the columns of the matrix, partition each column, load each partition, compute its sum, and then compute the sum of the partition sums.

[^11]: This may seem artificial, but suppose we were dealing with a very 'tall' matrix.

```{r}
# Here we will partition each column into two equal-sized subsets.
lib_sizes_loop_over_cols_subset <- vector("numeric", ncol(tenx_subset))
tmp_colsums <- vector("numeric", 2)
nr <- nrow(tenx_subset)

for (j in seq_len(ncol(tenx_subset))) {
  # A simple progress report.
  if (j %% 100 == 0) message("Processed ", j, "/", ncol(tenx_subset))
  for (i in 1:2) {
    i1 <- (i - 1) * nr / 2 + 1
    i2 <- i * nr / 2
    tmp_colsums[i] <- colSums(
      as.array(tenx_subset[seq(i1, i2), j, drop = FALSE]))
  }
  lib_sizes_loop_over_cols_subset[j] <- sum(tmp_colsums)
}
# Check we get the same result as before.
# (We'll ignore the names of the result)
all.equal(lib_sizes_loop_over_cols, lib_sizes, 
          check.attributes = FALSE)
```

Hopefully you can now begin to see the general pattern, a strategy which the `r BiocStyle::Biocpkg("DelayedArray")` package calls 'block-processing':

1. Load a 'block' of the data into memory.
2. Compute a summary statistic.
3. Combine the block-level statistics in an appropriate way to get the final result.

**TODO**: Demonstrate that this process has been abstracted away with `rowGrid()`/`colGrid()`,`rowAutoGrid()`, and `colAutoGrid()`.

#### Block-processing illustrated

Some examples of block-processing are illustrated in the following figures:

`r knitr::include_graphics("images/Slide1.png")`
`r knitr::include_graphics("images/Slide2.png")`
`r knitr::include_graphics("images/Slide3.png")`
`r knitr::include_graphics("images/Slide4.png")`
`r knitr::include_graphics("images/Slide5.png")`
`r knitr::include_graphics("images/Slide6.png")`
`r knitr::include_graphics("images/Slide7.png")`

#### Block-processed column sums

When we run `colSums(tenx_subset`) we are using a block-processed version of column sums, specifically the `colSums,DelayedMatrix-method` implemented in the `r BiocStyle::Biocpkg("DelayedArray")` package.
We can see this more clearly by turning on verbose progress reporting from the `r BiocStyle::Biocpkg("DelayedArray")` package.

```{r}
# Turn on progress reports for DelayedArray's block-processing routines.
DelayedArray:::set_verbose_block_processing(TRUE)
head(colSums(tenx_subset))
```

#### More examples of block-processed operations in `r BiocStyle::Biocpkg("DelayedArray")`

Some of the most useful functions in the `r BiocStyle::Biocpkg("DelayedArray")` package implement common operations on a *DelayedMatrix* using block-processing.
These include the following row and column summarization methods:

- `rowSums()`
- `colSums()`
- `rowMeans()`
- `colMeans()`
- `rowMaxs()`
- `colMaxs()`
- `rowMins()`
- `colMins()`
- `rowRanges()`
- `colRanges()`

Two useful but lesser known functions use block-processing to compute column/row sums of a *DelayedMatrix* based on a grouping variable:

- `rowsum()`
- `colsum()`

Finally, matrix multiplication is implemented as a block-processed operation.

```{r}
# This is mathematically equivalent to rowSums(tenx_subset).
# TODO: Is this still using block-processing?
tenx_subset %*% matrix(1, nrow = ncol(tenx_subset))
```

#### DelayedMatrixStats

We've already seen the `r BiocStyle::Biocpkg("DelayedMatrixStats")` package in action back when computing the [Median expression of each gene].
`r BiocStyle::Biocpkg("DelayedMatrixStats")` is a port of the `r BiocStyle::CRANpkg("matrixStats")` package's API for use with *DelayedMatrix* objects.
It provides [more than 70 functions](https://github.com/PeteHaitch/DelayedMatrixStats#api-coverage) that apply to rows and columns of *DelayedMatrix* objects.

**TODO**: Include overview of API from README.

#### General block-processing

As we have seen, many common block-processing operations on a *DelayedMatrix* have already been implemented in `r BiocStyle::Biocpkg("DelayedArray")` or are provided by `r BiocStyle::Biocpkg("DelayedMatrixStats")`.
Nonetheless, there may be times you need to implement your own algorithm using block-processing.
The documentation on this topic is a little sparse, but some details can be found in `help("block_processing", "DelayedArray"),` `help("ArrayGrid", "DelayedArray")`, `help("blockApply", "DelayedArray")`, and `help("AutoGrid", "DelayedArray")` or by reading the source code of the aforementioned packages. 
Briefly, to perform block-processing requires that you:

1. Set up an *ArrayGrid* over your *DelayedArray*. This specifies the 'block' structure that will be traversed when processing the *DelayedArray*.
2. Iterate over the *DelayedArray* via the *ArrayGrid*
    a.  Read each block of data into memory as an ordinary array using `read_block()`. **TODO**: Mention that sparse data can be read in as sparse array.
    b.  Compute the statistic for that block
3. Appropriately combine the block-level statistics to get your final result.

The `blockApply()` and `blockReduce()` functions can help facilitate steps 1-3 , even incorporating parallelization via the `r BiocStyle::Biocpkg("BiocParallel")` package.

### Realization

To *realize* a *DelayedArray* object is to trigger execution of the delayed operations carried by the object and return the result as an ordinary array **TODO**: or as a sparse array.
One way to achieve this is to call `as.array()` on it.

```{r}
tenx_subset_realized <- as.array(tenx_subset)
tenx_subset_realized[1:10, 1:10]
```

However, this realizes the full object at once in memory which could require too much memory if the object is big[^12].

[^12]: In the above example it's safe because `tenx_subset_realized` only requires around 100 Mb of memory.

A large *DelayedArray* object is preferably realized on disk, which is most commonly an HDF5 file **TODO:** Or TileDB file.

**TODO:** `as(tenx_subset, "dgCMatrix")`

#### Realizing to an HDF5 file

Realizing to an HDF5 file requires that the `r BiocStyle::Biocpkg("HDF5Array")` package is installed[^13]

[^13]: The `r BiocStyle::Biocpkg("TENxBrainData")` package depends upon `r BiocStyle::Biocpkg("HDF5Array")`, so it is already installed. If you've been following this workflow then you've also already attached it to the search path by running `library(TENxBrainData)`. **TODO**: Check this is true (I think I'm no longer using the TENxBrainData).

```{r}
tenx_subset_hdf5 <- writeHDF5Array(tenx_subset)
tenx_subset_hdf5

# Alternatively, we could 'coerce' the result to be an HDF5Array.
as(tenx_subset, "HDF5Array")
```

Notice that this process of realization used block-processing, which avoids loading the entire dataset entire memory.
Also notice that the result of realization is here returned as an *HDF5Matrix*, which no longer carries around the delayed operations.

```{r}
class(tenx_subset)
class(tenx_subset_hdf5)

showtree(tenx_subset)
showtree(tenx_subset_hdf5)
```

Used like this, `writeHDF5Array()` and `as(..., "HDF5Array")` will write their results to a file in *HDF5 dump directory*, a dumping ground for automatically created HDF5 datasets.
We can see a log of the operations that have written to the HDF5 dump directory using `showHDF5DumpLog()`.

```{r}
showHDF5DumpLog()
```

Often, however, we will want full control of where and how the data are written to the HDF5 file[^14] and the `writeHDF5Array()` function gives you full control over this and more.

[^14]: We'll discuss why you might want this later in the workshop.

```{r}
# Write the data to a user-specified HDF5 file using maximum compression and 
# 'chunking' along the columns.
my_hdf5_file <- tempfile(fileext = ".h5")
tenx_subset_my_file_hdf5 <- writeHDF5Array(
  tenx_subset,
  filepath = my_hdf5_file,
  chunkdim = c(nrow(tenx_subset), 1),
  level = 9)

# Compare `tenx_subset_hdf5` to `tenx_subset_my_file_hdf5`
seed(tenx_subset_hdf5)
seed(tenx_subset_my_file_hdf5)
```

#### Realization backends

We've now seen that we can realize to an HDF5 file. This is called as the HDF5Array 'realization backend' and is implemented in the `r BiocStyle::Biocpkg("HDF5Array")` package.
There are a few other realization backends to be aware of.

```{r}
supportedRealizationBackends()
```

There is also the `NULL` backend, which means the data are realized in memory as an ordinary *array* and then wrapped in a *DelayedArray*.
This is the default realization backend upon loading/attaching the `r BiocStyle::Biocpkg("DelayedArray")` package.

```{r}
getRealizationBackend()
```

The default realization backend can be altered with `setRealizationBackend()`.

```{r}
setRealizationBackend("HDF5Array")
getRealizationBackend()
setRealizationBackend(NULL)
getRealizationBackend()
```

It can be important to know what your current realization backend is because it will be used implicitly by some functions.
For example, matrix multiplication that involves a *DelayedMatrix* uses the current realization backend.

```{r}
setRealizationBackend("HDF5Array")
tenx_subset %*% matrix(1, nrow = ncol(tenx_subset))

setRealizationBackend(NULL)
tenx_subset %*% matrix(1, nrow = ncol(tenx_subset))
```

#### The `realize()` function

We've seen that we can realize to the HDF5Array backend using `writeHDF5Array(tenx_subset)` and `as(tenx_subset, "HDF5Array")`.
A third way of realizing a *DelayedArray* to an HDF5 file is with the `realize()` function.

```{r}
realize(tenx_subset, BACKEND = "HDF5Array")
```

So why might you use `realize()` instead of these other options?
Because it allows us to easily switch out the realization backend and will defer to the current realization backend if none is supplied.

```{r}
# Realize to the current realization backend.
getRealizationBackend()
realize(tenx_subset)

# TODO: This example is broken.
# # Realize as an RleArray.
# setRealizationBackend("RleArray")
# tenx_subset_rlearray <- realize(tenx_subset)
# tenx_subset_rlearray

# Switch back to the default backend.
setRealizationBackend(NULL)
```

This is probably most useful when writing package code so that you can allow the user control over the realization backend.

## Package ecosystem

**TODO**: Incorporate into intro intro slides?

The DelayedArray framework is, unsurprisingly, implemented in the `r BiocStyle::Biocpkg("DelayedArray")` package.
However, there are several other key packages that are an important part of the broader 'ecosystem'.
More importantly, as a user of Bioconductor software, it is increasingly likely that you will encounter *DelayedArray* objects during a data analysis, especially if you are analysing single-cell data[^15].
The following table lists packages that depend upon the `r BiocStyle::Biocpkg("DelayedArray")` package.

[^15]: In fact, if you use any package that makes use of the *SummarizedExperiment* class, then you will almost certainly load the `r BiocStyle::Biocpkg("DelayedArray")` package during the course of your analysis, whether you know it or not! This is because `r BiocStyle::Biocpkg("SummarizedExperiment")` depends upon `r BiocStyle::Biocpkg("DelayedArray")`.

```{r}
dep_tbl <- BiocPkgTools::buildPkgDependencyDataFrame()
da_dep_tbl <- dep_tbl[dep_tbl$dependency == "DelayedArray", 
                      c("Package", "edgetype")]
da_dep_tbl <- da_dep_tbl[with(da_dep_tbl, order(edgetype, Package)), ]
colnames(da_dep_tbl) <- c("Package", "Dependency Type")
rmarkdown::paged_table(da_dep_tbl)
```

We will briefly highlight some of the key packages in this table, broadly categorising these as 'user-focused'/'user-facing' or 'developer-focused' packages and those that span the spectrum.

### Packages that both users and developers should probably know about

#### `r BiocStyle::Biocpkg("DelayedArray")`

Well, duh.
Implements the *DelayedArray* and *RleArray* classes, along with all the fundamentals the enable the delayed operations, block processing, and realization that underpin the DelayedArray framework.

#### `r BiocStyle::Biocpkg("HDF5Array")`

Implements the *HDF5Array* and *TENxMatrix* classes, two convenient and memory-efficient array-like containers for on-disk representation of HDF5 datasets. 
*HDF5Array* is for datasets that use the conventional (i.e. dense) HDF5 representation.
*TENxMatrix* is for datasets that use the HDF5-based sparse matrix representation from 10x Genomics.

#### `r BiocStyle::Githubpkg("LTLA/TileDBArray")`

Implements a DelayedArray backend for [TileDB](https://tiledb.com/) to read, write and store dense and sparse arrays.
The resulting *TileDBArray* objects are directly compatible with any Bioconductor package that accepts *DelayedArray* objects, serving as a swap-in replacement for the predominant *HDF5Array* that is currently used throughout the Bioconductor ecosystem for representing large datasets.

**NB**: `r BiocStyle::Githubpkg("LTLA/TileDBArray")` is not yet available from Bioconductor.

#### `r BiocStyle::Biocpkg("DelayedMatrixStats")`

A port of the `r BiocStyle::CRANpkg("matrixStats")` API for use with *DelayedMatrix* objects.
High-performing functions operating on rows and columns of *DelayedMatrix* objects, e.g. `col` / `rowMedians()`, `col` / `rowRanks()`, and `col` / `rowSds()`.
Functions optimized per data type and for subsetted calculations such that both memory usage and processing time is minimized. 

Disclaimer: I wrote this one.

#### `r BiocStyle::Biocpkg("BiocSingular")`

Implements exact and approximate methods for singular value decomposition and principal components analysis using a framework that allows them to be easily switched within Bioconductor packages or workflows.
These methods work on *DelayedMatrix* objects as well as ordinary *matrix* objects and some sparse matrix objects from the `r BiocStyle::CRANpkg("Matrix")` package.

**TODO**: Mention *DeferredMatrix*, *LowRankMatrix*, and *ResidualMatrix*.

#### `r BiocStyle::Biocpkg("VCFArray")` and `r BiocStyle::Biocpkg("GDSArray")`

Implements the *VCFArray* and *GDSArray* classes, types of *DelayedArray*, to represent VCF files and GDS-files in an array-like representation. VCF and GDS files are widely used to represent genotyping or sequence data.

#### `r BiocStyle::Biocpkg("rhdf5client")` and `r BiocStyle::Biocpkg("restfulSE")`

Provide functions and classes to interface with remote data stores by operating on *SummarizedExperiment*-like objects.
These data are HDF5 files living on a remote server running `h5serv`, a REST-based service for HDF5 data.

### User-focused/user-facing packages

These are the packages that as a user you might directly load/attach with `library()` as part of a data analysis.
Alternatively, these may be loaded/attached as a dependency[^16] of another package you load/attach as part of an analysis.

[^16]: As listed in the `Depends` field of the package `DESCRIPTION` file.

#### `r BiocStyle::Biocpkg("DropletUtils")`

Provides a number of utility functions for handling single-cell (RNA-seq) data from droplet technologies such as 10X Genomics.
This includes `read10xCounts()` for data loading from the count matrices produced by 10x Genomics' **CellRanger** software, which may be stored in an HDF5 file. 
To do this, it makes use of the *TENxMatrix* class.

#### `r BiocStyle::Biocpkg("LoomExperiment")`

Provides a means to convert from 'loom' files to standard Bioconductor classes and back again.
The [Loom file format](http://linnarssonlab.org/loompy/index.html) uses HDF5 to store experimental data and is used by some tools and labs producing data using single-cell assays.
This includes the `import()` function for data loading from loom files into an *HDF5Matrix*.

#### `r BiocStyle::Biocpkg("scuttle")`

Provides basic utility functions for performing single-cell analyses, focusing on simple normalization, quality control and data transformations.
Also provides some helper functions to assist development of other packages.
These methods work on *DelayedMatrix* objects as well as ordinary *matrix* objects and some sparse matrix objects from the `r BiocStyle::CRANpkg("Matrix")` package.

#### `r BiocStyle::Biocpkg("batchelor")`

Implements a variety of methods for batch correction of single-cell (RNA sequencing) data, such as `multiBatchPCA()` and `fastMNN()`.
These methods work on *DelayedMatrix* objects as well as ordinary *matrix* objects and some sparse matrix objects from the `r BiocStyle::CRANpkg("Matrix")` package.

#### `r BiocStyle::Biocpkg("bsseq")`

A collection of tools for analyzing and visualizing bisulfite sequencing data.
This was one of the first packages to make use of the DelayedArray framework and it supports these throughout the package.
This was needed in order to store and analyse large non-CpG methylation datasets (\> 300 million loci, hundreds of samples) using HDF5 files.

Disclaimer: I did this re-write of `r BiocStyle::Biocpkg("bsseq")` and learnt a lot along the way.

#### `r BiocStyle::Biocpkg("minfi")`

Tools to analyze & visualize Illumina Infinium methylation arrays.
This doesn't have the same level of support for *DelayedMatrix* objects as `r BiocStyle::Biocpkg("bsseq")`, but perhaps one day.
This is needed in order to store and analyse large methylation datasets (\> 850,000 loci, tens of thousands of) using HDF5 files.

Disclaimer: This was the second package, after `r BiocStyle::Biocpkg("bsseq")`, I started to re-write to support the DelayedArray framework. 
Here, it is rather more difficult because it is a 'widely' used package and has code from lots of different authors with different styles.

### Developer-focused packages

#### `r BiocStyle::Biocpkg("beachmat")`

Provides a consistent C++ class interface for reading from and writing data to a variety of commonly used matrix types.
Ordinary matrices and several sparse/dense `r BiocStyle::CRANpkg("Matrix")` classes are directly supported, third-party S4 classes may be supported by external linkage (such as the *HDF5Matrix* class), while all other matrices are handled by DelayedArray block processing.

## Workflow tips for DelayedArray-backed analyses

We'll conclude with some miscellaneous tips I've collected over the past few years of using DelayedArray-backed workflows.

To demonstrate, we'll create a *SingleCellExperiment* object containing a subset of the `tenx` counts data.

```{r}
library(SingleCellExperiment)
sce <- SingleCellExperiment(assays = list(counts = tenx[, 1:10000]))
```

We term this an HDF5-backed SummarizedExperiment because:

1. A *SingleCellExperiment* is (a derivative of) a *SummarizedExperiment*.
2. The assay data are stored in an HDF5 file.

```{r}
sce
# We'll discuss the use of `withDimnames = FALSE` shortly.
assay(sce, withDimnames = FALSE)
```

To make the example a little bit more interesting, we'll also normalize the data.

```{r}
library(scuttle)
sce <- computeLibraryFactors(sce)
sce <- logNormCounts(sce)
```

The resulting *SingleCellExperiment* object contains two assays - `counts` and `logcounts` - both of which are *DelayedMatrix* objects.

```{r}
sce
assay(sce, "counts", withDimnames = FALSE)
assay(sce, "logcounts", withDimnames = FALSE)
```

### Saving and loading HDF5-backed SummarizedExperiment objects

#### Short version

Use `HDF5Array::saveHDF5SummarizedExperiment()` and `HDF5Array::loadHDF5SummarizedExperiment()` rather than `saveRDS()` and `readRDS()` or `save()` and `load()`.
Calling `HDF5Array::saveHDF5SummarizedExperiment()` will realize any delayed operations prior to saving the assay data in an HDF5 file, as illustrated below.

```{r}
dir <- file.path(tempdir(), "my_h5_se")
saveHDF5SummarizedExperiment(sce, dir, verbose = TRUE)

# There are no delayed operations on the saved version.
saved_sce <- loadHDF5SummarizedExperiment(dir)
showtree(logcounts(sce, withDimnames = FALSE))
showtree(logcounts(saved_sce, withDimnames = FALSE))
```

**TODO**: Discuss `quickResaveHDF5SummarizedExperiment()`.

#### Long version

A HDF5-backed *SummarizedExperiment*, like the 10x PBMC dataset we analysed in [Real world encounter with DelayedArray analysing scRNA-seq data], is a light-weight shell (the *SummarizedExperiment*) around a large disk-backed data matrix (the *HDF5Matrix*).
**TODO**: This is out of date because I've removed the referenced section.
The following explanation comes from `help("saveHDF5SummarizedExperiment", "HDF5Array")`:

Roughly speaking, `saveRDS()` only serializes the part of an object that resides in memory[^17].
For most objects in R, that's the whole object, so `saveRDS()` does the job.

[^17]: The reality is a little bit more nuanced, but discussing the full details is not important here, and would only distract us

However some objects are pointing to on-disk data. For example:

- A *TxDb* object from the `r BiocStyle::Biocpkg("GenomicFeatures")` points to an SQLite db
- An *HDF5Array* object points to a dataset in an HDF5 file
- A *SummarizedExperiment* derivative can have one or more of its assays that point to datasets (one per assay) in an HDF5 file.

These objects have 2 parts: one part is in memory, and one part is on disk.
The 1st part is sometimes called the object shell and is generally thin (i.e. it has a small memory footprint).
The 2nd part is the data and is typically big. 
The object shell and data are linked together via some kind of pointer stored in the shell (e.g. an SQLite connection, or a path to a file, etc.).
Note that this is a one way link in the sense that the object shell "knows" where to find the on-disk data but the on-disk data knows nothing about the object shell (and is completely agnostic about what kind of object shell could be pointing to it).
Furthermore, at any given time on a given system, there could be more than one object shell pointing to the same on-disk data.
These object shells could exist in the same R session or in sessions in other languages (e.g. Python).
These various sessions could be run by the same or by different users.

Using `saveRDS()` on such object will only serialize the shell part so will produce a small `.rds` file that contains the serialized object shell but not the object data.

This is problematic because:

1.  If you later unserialize the object (with `readRDS()`) on the same system where you originally serialized it, it is possible that you will get back an object that is fully functional and semantically equivalent to the original object. But here is the catch: this will be the case **ONLY** if the data is still at the original location and has not been modified (i.e. nobody wrote or altered the data in the SQLite db or HDF5 file in the mean time), and if the serialization/unserialization cycle didn't break the link between the object shell and the data (this serialization/unserialization cycle is known to break open SQLite connections).
2.  After serialization the object shell and data are stored in separate files (in the new `.rds` file for the shell, still in the original SQLite or HDF5 file for the data), typically in very different places on the file system. But these 2 files are not relocatable, that is, moving or copying them to another system or sending them to collaborators will typically break the link between them. Concretely this means that the object obtained by using `readRDS()` on the destination system will be broken.

`saveHDF5SummarizedExperiment()` addresses these issues by saving the object shell and assay data in a folder that is relocatable.

Note that it only works on *SummarizedExperiment* derivatives. What it does exactly is:

1.  Write all the assay data to an HDF5 file
2.  Serialize the object shell, which in this case is everything in the object that is not the assay data.

The 2 files (HDF5 and `.rds`) are written to the directory specified by the user.
The resulting directory contains a full representation of the object and is relocatable, that is, it can be moved or copied to another place on the system, or to another system (possibly after making a tarball of it), where `loadHDF5SummarizedExperiment()` can then be used to load the object back in R.

**TODO**: Discuss `quickResaveHDF5SummarizedExperiment()`.

### Avoid subsetting too much and random access patterns

Reordering/subsetting the data may degrade the performance of even seemingly simple operations.
This is especially true of disk-backed data, where performance is best when reading contiguous chunks of data and worst when having to read data with a random access pattern.

```{r}
x <- counts(sce, withDimnames = FALSE)
y <- x[sample(nrow(x)), sample(ncol(x))]
system.time(colSums(x))
system.time(colSums(y))
```

### Process, save, repeat

When analysing large datasets, a workflow that is broken up into stages and saves the intermediate outputs can be help preserve one's sanity.
This is true regardless of whether the DelayedArray framework is used - it sucks having to repeat some long pre-processing computation in order to make a quick plot - but it is especially true for DelayedArray-backed analyses where the accumulation of delayed operations will eventually lead to degraded performance. 
This means using `saveHDF5SummarizedExperiment()`/`loadHDF5SummarizedExperiment()`, or `realize()`-ing the result if there is sufficient memory available, following time consuming processing of the object.

### Pragmatism rules

**TODO**: Needs a re-write.

We've seen some examples of pragmatism in this workshop: just load the data into memory, compute the thing you need, and move on.
For the workshop, I used it to 'cheat' in order to speed things up, but it is frequently a valid strategy when analysing data!

For example, a normalized scRNA-seq dataset carries around two matrices: the raw counts and the normalized expression values.
You might have enough RAM to load one of these a time but not both at once. With a HDF5-backed *SingleCellExperiment* you can easily just load into memory the matrix you actually need at a given step in the analysis.

Another example, with WGBS data you carry around 3 very large matrices.
But to make a plot of methylation values along a gene promoter, a common requirement, you only need to load in a small 'slice' of one of these matrices.
With a HDF5-backed *BSseq*[^18] object you can quickly do this.

[^18]: A *SummarizedExperiment* derivative

This brings us to a perhaps underappreciated advantage of using HDF5-backed *SummarizedExperiment* derivatives, namely that loading the saved data with `HDF5Array::loadHDF5SummarizedExperiment()` is really fast.
I made extensive use of this when processing large WGBS datasets as I could quickly load the *BSseq* object to compute summaries of the sample metadata (stored in the `colData` of the *BSseq* object), a process that used to take tens of minutes to hours because the 3 large matrices also had to be loaded into memory.
This has been so useful to me that I now keep even 'small' WGBS datasets as HDF5-backed *BSseq* objects.

This pragmatism has served me well.
Often times I find myself starting with a very large dataset, do my initial processing using the disk-backed data, and run `saveHDF5SummarizedExperiment()` to produce a 'pristine' object.
I can then move it to a large-memory machine, do my big computation (e.g., PCA), save the result, and **move on with my life**.

### Avoid 'degrading' to a *DelayedArray*

The DelayedArray framework is implemented using the S4 object oriented system.
This can be used to write methods that are optimized for a particular backend. For example, we might write a `colMaxs()` method that is optimized for the *TENxMatrix* class by exploiting the sparse storage mode of the underlying data.
In order for `colMaxs()` to 'know' that it can use this optimized method, however, it needs for the data to be supplied as a *TENxMatrix* instance.

Unfortunately, it is very easy to 'degrade' a specialised *DelayedArray* derivative to a *DelayedArray*.

```{r}
# Let's create a TENxMatrix
tenx_matrix <- as(counts(sce), "TENxMatrix")
class(tenx_matrix)

# All these degrade the result to a DelayedMatrix.
class(tenx_matrix[, 1:10])
val <- tenx_matrix
dimnames(val) <- list(paste0("R", seq_len(nrow(val))), NULL)
class(val)
```

A common scenario where this degrading may occur is when extracting the data from a *SummarizedExperiment*.

```{r}
tmp <- SingleCellExperiment(
  list(counts = tenx_matrix),
  colData = DataFrame(row.names = paste0("S", seq_len(ncol(tenx_matrix)))))
class(assay(tmp))
```

What's happened here?
By default, `assay(tmp)` calls `assay(tmp, withDimnames = TRUE)` which has the effect of copying the dimnames from the *SummarizedExperiment* and adding them to the return assay data.
As we saw above, setting the dimnames on a *TENxMatrix* (or other *DelayedMatrix* derivative) will degrade it to a *DelayedMatrix*.
Consequently, running `colMaxs(assay(tmp))` will not call our (hypothetical) optimized method for *TENxMatrix* objects and will instead defer to the slower, more general block-processing method that is implemented for *DelayedMatrix* objects.

To avoid this 'degrading upon assay extraction', we can should set `withDimnames = FALSE`.

```{r}
class(assay(tmp, withDimnames = FALSE))
```

More generally, you may need to avoid degrading a *DelayedArray* derivative to a *DelayedArray* in order to use backend-optimized methods.

### Make use of sparsity

**TODO:** Demo the sparsity-preserving feature of the DelayedArray framework when used with a sparse array (e.g., the `tenx_matrix` *TENxMatrix* object).

### Block size

**TODO**: Move this up in prominence.

The maximum size of a block used when performing block-processing is given by `getAutoBlockSize()`.
By default, this is set to `r as.integer(getAutoBlockSize())` meaning each block can use up to `r as.integer(getAutoBlockSize())` / `1e6` = `r getAutoBlockSize() / 1e6` Mb of data.

Using fewer, larger blocks generally means faster performance (at the cost of higher peak memory usage).
Conversely, users more, smaller blocks generally means slower performance (at the benefit of lower peak memory usage).
Therefore, we may wish to increase/decrease this on machines with sufficient memory by using `setAutoBlockSize()`.

```{r}
# TODO: Check if these resutls mesh with the text.
system.time(colSums(counts(sce, withDimnames = FALSE)))

# Increasing the block size 10-fold.
setAutoBlockSize(getAutoBlockSize() * 10)
system.time(colSums(counts(sce, withDimnames = FALSE)))
# Reverting to default block size.
setAutoBlockSize(getAutoBlockSize() / 10)

# Decreasing the block size 10-fold
setAutoBlockSize(getAutoBlockSize() / 10)
system.time(colSums(counts(sce, withDimnames = FALSE)))
# Reverting to default block size.
setAutoBlockSize(getAutoBlockSize() / 10)
```

### Chunking

**TODO**: Move this up in prominence.

Data stored in an HDF5 file are usually 'chunked' into sub-matrices (for matrix data) or hyper-cubes (for arrays of higher dimension)[^19].
For example, we could choose to chunk an $R \times C$ matrix by column, by row, or into $r \times c$ sub-matrices ($r \leq R, c \leq C$).
This may remind you of the choice of block dimensions used in block-processing. The difference is this:

[^19]: Chunking also applies to other storage formats, such as *RleArray* objects, but we'll focus on the HDF5 format.

> **Block dimensions dictate how the data are accessed, chunk dimensions dictate how the data are stored**.

In general, you want your data to be chunked in a manner that supports the type of access patterns you will be making.
For example, if you know you only need to access data by column then chunk the data by column.
Of course, you often either don't know in advance what access patterns you need or you need both row and column access.
In that case, chunking into sub-matrices offers the best tradeoff.

We'll demonstrate how chunking can affect performance by comparing computing the column sums of a column-chunked and row-chunked dataset.

```{r}
# TODO: Check if these results mesh with the text.

x <- matrix(sample(1e8), ncol = 1e2, nrow = 1e6)
x_col <- writeHDF5Array(x, chunkdim = c(nrow(x), 1))
x_row <- writeHDF5Array(x, chunkdim = c(1, ncol(x)))

system.time(colSums(x_col))
system.time(colSums(x_row))
```

Chunking can be controlled by the `chunkdim` argument when writing data to disk using the `writeHDF5Array()` or `saveHDF5SummarizedExperiment()` functions.

**TODO**: Comment on similar functionality in TileDBArray?

### Parallelization strategies

Several of the routines we used in today's workshop have 'native' parallelization via the `r BiocStyle::Biocpkg("BiocParallel")` package.
Look out for the `BPPARAM` argument in function documentation[^20].

[^20]: Unfortunately, simply typing the function name at the command prompt or using `args()` may not be sufficient to identify these functions. E.g., `args(denoisePCA)` doesn't include `BPPARAM` but `help("denoisePCA", "scran")` tells you this is an option (Footnote to the footnote: this is because `denoicePCA()` is an S4 generic with methods defined only for the first argument, `x`. The methods include the `BPPARAM` argument and are documented as such.)

Fair warning: parallelization is tricky and performance may not match expectations.

### What's in my HDF5 file?

I do this quite a bit: I've got an HDF5 file but I don't know (or have forgotten) what's in it. `rhdf5::h5ls()` to the rescue.

```{r}
rhdf5::h5ls(path(counts(sce)))
```

## Things I didn't discuss

**TODO**: Include this section? If so, do a more systematic review and discussion.

- *RealizationSink*
- General I/O (e.g., how to create a HDF5Matrix from a bunch of files)
- Parallelization

## Concluding remarks

We've now spent nearly 1 hour learning about the DelayedArray framework.
Now comes the kicker:

> **Don't use a *DelayedArray* if you can avoid it!**

I write that as someone who has a professional love for the DelayedArray framework.
I've spent many, many hours developing software that supports and extends it, leveraging it to analyse large WGBS and scRNA-seq datasets, and writing and presenting talks and workshops to teach it. 
But the simple fact remains:

> **If you can load your data into memory and still compute on it, then you're always going to have a better time doing it that way.**

Analyses will be faster, simpler, and you will have more software options available to you.
But when this isn't an option then the DelayedArray framework is a powerful set of packages to help you get your work done.
For example, I find it pretty remarkable that a first-class single-cell analysis workflow can so seamlessly support the use of both in-memory and disk-backed data.

A huge amount of gratitude is owed to Hervé Pagès, the developer of the `r BiocStyle::Biocpkg("DelayedArray")` package and the broader framework, and Aaron Lun, the main developer many of the key packages for analysing scRNA-seq data that build upon the DelayedArray framework.
Thank you!

## Session info

This document was prepared using the following software:

```{r}
sessionInfo()
```

## References
