---
title: "Effectively using the DelayedArray framework as a user to support the analysis of large datasets"
subtitle: "Presented at BioC 2020 (27-31 July)"
author: "Peter Hickey"
date: "Last modified: July 28, 2020; Compiled: `r format(Sys.time(), '%B %d, %Y')`"
bibliography: "`r system.file(package = 'DelayedArrayWorkshop', 'vignettes', 'ref.bib')`"
output:
  rmarkdown::html_document:
   highlight: pygments
   toc: true
   toc_depth: 3
   fig_width: 5
vignette: >
  %\VignetteIndexEntry{Effectively using the DelayedArray framework as a user to support the analysis of large datasets}
  %\VignetteEncoding[ut8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
resource_files:
  - images/SummarizedExperiment.svg
  - images/*.png
---

```{r setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE)
```

## Workshop description

This workshop gives an introductory overview of the DelayedArray framework, which can be used by R / Bioconductor packages to support the analysis of large array-like datasets.
A *DelayedArray* is like an ordinary array in R, but allows for the data to be in-memory, on-disk in a file, or even hosted on a remote server.

Workshop participants will learn where they might encounter a *DelayedArray* in the wild while using Bioconductor and help them understand the fundamental concepts underlying the framework.
This workshop will feature [introductory material](https://docs.google.com/presentation/d/1_v4IuKLFs781pp7x0BwUxv7mwCfPLBTWu320g3lQmME/edit?usp=sharing), 'live' coding, and Q&A.

### Instructor

- [Peter Hickey](https://peterhickey.org/) (hickey@wehi.edu.au)

### Pre-requisites

- Basic knowledge of R syntax.
- Familiarity with common operations on matrices in R, such as `colSums()` and `colMeans()`.
- Some familiarity with S4 objects may be helpful but is not required.

### Workshop Participation

Students will be able to run code examples from the workshop material.
There will be a Q&A session in the second half of the workshop.

### *R* / *Bioconductor* packages used

These packages are the focus of this workshop:

- `r BiocStyle::Biocpkg("DelayedArray")`
- `r BiocStyle::Biocpkg("HDF5Array")`
- `r BiocStyle::Biocpkg("DelayedMatrixStats")`

Please see the workshop `DESCRIPTION` for a full list of dependencies.

### Time outline

| Activity                                       | Time   |
|------------------------------------------------|--------|
| Introductory material                          | 5 min  |
| First contact                                  | 20 min |
| Workflow tips for DelayedArray-backed analyses | 15 min |
| Q&A                                            | 15 min |

### Workshop goals and objectives

#### Learning goals

- Learn of existing packages and functions that *use* the DelayedArray framework.
- Develop a high-level understanding of classes and packages that *implement* the DelayedArray framework.
- Become familiar with the fundamental concepts of delayed operations, block processing, and realization.
- Reason about potential bottlenecks, and how to avoid or reduce these, in algorithms operating on *DelayedArray* objects.

#### Learning objectives

- Identify when an object is a *DelayedArray* or one of its derivatives.
- Be able to recognise when it is useful to use a *DelayedArray* instead of an ordinary array or other array-like data structure.
- Learn how to load and save a DelayedArray-backed object.
- Learn how the 'block size' and 'chunking' of the dataset affect performance when operating on *DelayedArray* objects.
- Take away some miscellaneous tips and tricks I've learnt over the years when working with DelayedArray-backed objects.

## Introductory material

Data from a high-throughput biological assay, such as single-cell RNA-sequencing (scRNA-seq), will often be summarised as a matrix of counts, where rows correspond to features and columns to samples[^1].
Within **Bioconductor**, the *SummarizedExperiment* class is the recommended container for such data, offering a rich interface that tightly links assay measurements to data on the features and the samples.

[^1]: Higher-dimensional arrays may be appropriate for some types of assays.

The *SummarizedExperiment* class is used to store rectangular arrays of experimental results (*assays*). Here, each *assay* is drawn as a matrix but higher-dimensional arrays are also supported.

`r knitr::include_graphics("images/SummarizedExperiment.svg")`

Traditionally, the assay data are stored in-memory as an ordinary *array* object[^2]. 
Storing the data in-memory becomes a real pain with the ever-growing size of 'omics datasets. 
It is now not uncommon to collect $10,000-100,000,000$ measurements on $100 - 1,000,000$ samples, which would occupy $10-1,000$ gigabytes (Gb) if stored in-memory as ordinary R arrays.

[^2]: In R, a *matrix* is just a 2-dimensional *array*

The DelayedArray framework offers a solution to this problem. 
Wrapping an array-like object (typically an on-disk object) in a *DelayedArray* object allows one to perform common array operations on it without loading the object in memory. 
In order to reduce memory usage and optimize performance, operations on the object are either delayed or executed using a block processing mechanism.

### Projects enabled by DelayedArray

The DelayedArray framework enables the analysis of datasets that are too large to be stored or processed in-memory.
This has become particularly relevant with the advent of large single-cell RNA-sequencing (scRNA-seq) studies containing tens of thousands to millions of cells.

In my own research I have made extensive use of the DelayedArray framework when analysing whole genome bisulfite sequencing (WGBS) datasets.
To give a recent example, we profiled 45 human brain samples using WGBS to measure DNA methylation [@rizzardi2019neuronal].
For most tissues, we focus on so-called so-called CpG methylation, which requires analysing matrices with roughly 20 million rows (CpG loci) and 45 columns (samples).
This sized data is challenging, but well within the realms of high performance computing available at a modern research institute.
However, the brain also has extensive non-CpG methylation and there are an order of magnitude more non-CpG loci.
This necessitated extensive re-factoring of our software tools and we successfully adopted the DelayedArray framework to enable this research.

## First contact

### Motivation

The heart of the DelayedArray framework is implemented in the `r BiocStyle::Biocpkg("DelayedArray")` package, which we now load and attach.

```{r}
library(DelayedArray)
```

We'll also load and attach the `r BiocStyle::Biocpkg("HDF5Array")` package, which extends the DelayedArray framework to support on-disk HDF5 files.

```{r}
library(HDF5Array)
```

We will begin with an example using some scRNA-seq data on 1.3 million brain cells from embryonic mice, generated by 10X Genomics.
This dataset is available from `r BiocStyle::Biocpkg("ExperimentHub")`^[This dataset is also available in the `r BiocStyle::Biocpkg("TENxBrainData")` Bioconductor package.].

```{r}
library(ExperimentHub)
hub <- ExperimentHub()

# Query ExperimentHub to find the relevant resource.
# This dataset is available in two formats: a 'dense matrix' format and a
# 'HDF5-based 10X Genomics' format. We'll use the 'dense matrix' version for 
# this workshop.
query(hub, "TENxBrainData")
# Load the relevant resource.
# This will download the data and may take a little while on the first run. 
# The result will be cached, however, so subsequent runs avoid re-downloading 
# the data.
fname <- hub[["EH1040"]]

# The structure of this HDF5 file can be seen using the h5ls() command
# from the rhdf5 package:
library(rhdf5)
h5ls(fname)

# The 1.3 Million Brain Cell Dataset is represented by the "counts" group. 
# We point the HDF5Array() constructor to this group to create a HDF5Matrix 
# object representing the dataset:
tenx <- HDF5Array(filepath = fname, name = "counts")

# The data contain counts on nearly 28,000 gene for more than 
# 1.3 million cells.
dim(tenx)
```

With data from 1.3 million cells, this is roughly 100,000-times more samples than a typical bulk RNA-seq dataset and would require over 140 GB of RAM to hold as a matrix and around 30 GB as a sparse matrix.

With so much data, we might expect that it would feel sluggish to interact with this object, but this is not the case.
For example, let's do something that would ordinarily be a terrible idea, and something that's frustrated me way too many times: let's 'accidentally' print out the entire counts matrix.

```{r}
tenx
```

Hallelujah!
Unlike what you may have experienced when printing out a large matrix, this didn't overwhelm the screen with thousands of lines of output nor did it cause the R session to hang indefinitely.
In fact, this gives us a rather pretty printing of the counts matrix[^3].
No need for panicked mashing of `Ctrl-c` or `Esc`.

[^3]: You may have seen similar pretty printing with other Bioconductor objects such as *GRanges* and *DataFrame* or with the *data.table* and *tibble* extensions to the *data.frame*. I can't say enough how much I appreciate these thoughtful touches when doing interactive data analysis.

#### A peak behind the curtain

We might now recognise that `tenx` is no ordinary *matrix*.
In fact, it is an *HDF5Matrix*, which is a type of *DelayedArray*[^4].

[^4]: As with a 2-dimensional *array* in base R being commonly known as a *matrix*, a 2-dimensional *DelayedArray* is also known as a *DelayedMatrix* and a 2-dimensional *HDF5Array* is also known as a *HDF5Matrix*.

```{r}
class(tenx)
is(tenx, "DelayedArray")
```

The data contained in an *HDF5Matrix* is actually stored on disk in a [Hierarchical Data Format (**HDF5**)](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) file.
Consequently, the `tenx` object takes up relatively little space in memory^[Most of the object's size is due to the dimnames being stored in memory.].

```{r}
print(object.size(tenx), units = "auto")
```

We can learn more about the internals of the `tenx` object using the `seed()` function.

```{r}
seed(tenx)
```

### Three examples of computing on a DelayedArray

We will now play around with computing on the counts matrix.
To make things slightly easier, we will first subset the data to 1000 samples.

```{r}
tenx_subset <- tenx[, 1:1000]
```

#### Library sizes

Firstly, let's compute the library sizes for this subset of samples.
We can do this using `colSums()`.

```{r}
lib_sizes <- colSums(tenx_subset)
summary(lib_sizes)
```

#### Proportion of cells with non-zero expression for each gene

Secondly, suppose we want to know for each gene the proportion of cells with non-zero expression.
We can do this using `rowSums()` in conjunction with some standard R commands (logical comparisons and division).

```{r}
prop_non_zero <- rowSums(tenx_subset > 0) /  ncol(tenx_subset)
summary(prop_non_zero)
```

#### Median expression of each gene

Finally, suppose we want to know the median expression of each gene.
Here, we will quantify expression as counts per million (CPM) using library size normalization.

```{r}
cpm <- t(t(1e6 * tenx_subset) / lib_sizes)
cpm
```

We can then compute the median expression of each gene using `DelayedMatrixStats::rowMedians()`.

```{r}
library(DelayedMatrixStats)
median_expression <- rowMedians(cpm)
summary(median_expression)
```

#### Summary

These 3 examples highlight the power of the DelayedArray framework.
Recall that the data in these examples live on disk in an HDF5 file, yet we interacted with `tenx_subset` and computed on it much as we would if the data were in-memory as an ordinary matrix.
Also note that all 3 examples returned ordinary R vectors.

```{r}
class(lib_sizes)
class(prop_non_zero)
class(median_expression)
```

To do so, we made (implicit) use of the three fundamental concepts of the DelayedArray framework:

1. Delayed operations
2. Block processing
3. Realization

We'll now discuss each of these in turn.

### Delayed operations

Taking a careful look at `tenx_subset`, we see that it is a *DelayedMatrix* rather than an *HDF5Matrix*.

```{r}
tenx_subset
```

The subsetting operation has 'degraded' the `tenx_subset` object to a *DelayedMatrix*.

```{r}
is(tenx_subset, "HDF5Matrix")
is(tenx_subset, "DelayedMatrix")
```

The `showtree()` function can help us see what changed when we subsetted the data.

```{r}
showtree(tenx)
showtree(tenx_subset)
```

The subsetting operation has been registered in what is termed a 'delayed operation'.
Registering a delayed operation does not modify the underlying data.
Instead, the operation is recorded and only performed when the *DelayedArray* object is 'realized'.
Realization of a *DelayedArray* triggers the execution of the delayed operations carried by the object and returns the result as an ordinary *array*.

This allows us to chain together multiple operations and only perform them as required.
Here is a contrived example.

```{r}
# Add 1 to every element (a delayed op).
x <- tenx_subset + 1L
showtree(x)

# Compute log of every element (another delayed op).
lx <- log(x)
showtree(lx)

# Transpose the result (another delayed op).
tlx <- t(lx)
showtree(tlx)

# Realize a subset of the data as an ordinary matrix.
as.array(tlx[1:5, 1:10])
```

Many common operations can be registered as delayed operations.
Here are some examples[^6].
Notice that in each case the result is 'degraded' to a *DelayedMatrix*[^7].

[^6]: The technical names of each type of delayed operation are not important.

[^7]: The [No-op] example is the obvious exception

#### DelayedSubset

```{r}
val <- tenx[, 1:100]
val
showtree(val)
```

#### DelayedAperm

```{r}
val <- t(tenx)
val
showtree(val)
```

#### DelayedUnaryIsoOp

```{r}
val <- tenx + 1L
val
showtree(val)
val <- tenx + 1:2
val
showtree(val)
```

#### DelayedSubassign

```{r}
tmp <- tenx
tmp[1, ] <- sample(10, ncol(tmp), replace = TRUE)
tmp
showtree(tmp)
```

**WARNING**: Be careful with delayed subassignment because you can end up with objects that are surprisingly large in-memory.
This is because the subassigned values are kept in-memory until the data are *realized*.

#### DelayedDimnames

```{r}
tmp <- tenx
rownames(tmp) <- paste0("R", seq_len(nrow(tmp)))
tmp
showtree(tmp)
```

#### DelayedNaryIsoOp

```{r}
val <- tenx + tenx
val
showtree(val)
```

#### DelayedAbind

```{r}
val <- cbind(tenx, tenx)
val
showtree(val)
```

#### No-op

The DelayedArray framework is smart enough to recognise that some combinations of operations are 'no-ops'.

```{r}
val <- t(t(tenx))
val
showtree(val)
```

But it can be fooled.

```{r}
# This is a no-op but DelayedArray doesn't recognise it as one.
val <- tenx + 0L
val
showtree(val)
```

### Block processing

In [Library sizes], we computed the column sums of `tenx_subset` by running `colSums(tenx_subset`).
If you have used the `colSums()` function in your own R work, then this code likely looks familiar to you.
However, recall that the `tenx_subset` data live on disk in an HDF5 file, so how did `colSums()` know how to handle this?

The 'trick' is that we are using a specialised version of `colSums()`^[Specifically, `DelayedArray::colSums()`.] which uses a technique called 'block processing' to compute the column sums.

#### Illustration of block processing

Block processing involves 2 steps: 

1. Load a 'block' of the data into memory and compute a statistic(s) on the block.
2. Combine the block-level statistics in an appropriate way to get the final result.

Some examples of block processing are illustrated in the following figures:

`r knitr::include_graphics("images/Slide1.png")`
`r knitr::include_graphics("images/Slide2.png")`
`r knitr::include_graphics("images/Slide3.png")`
`r knitr::include_graphics("images/Slide4.png")`
`r knitr::include_graphics("images/Slide5.png")`
`r knitr::include_graphics("images/Slide6.png")`
`r knitr::include_graphics("images/Slide7.png")`

For example, to compute the column sums we could define a block to be a column, loop over the blocks (columns), load each block (column) into memory, and compute it's sum.
You may already be thinking:

- "I have a very 'tall' matrix and I can't load even a single column into memory. Can block processing support this?"
- "I have enough RAM to load 100 columns of my matrix into memory. Can block processing support this?"

The answer to both these questions is generally "yes", which we will return to in the [Block size] section.

### More examples of operations that use block processing

To more clearly see block processing in action, we'll turn on verbose progress reporting from the `r BiocStyle::Biocpkg("DelayedArray")` package.

```{r}
DelayedArray:::set_verbose_block_processing(TRUE)
```

Now, let's re-run `colSums(tenx_subset)`.

```{r}
# invisible() is used to prevent the result from printing to the screen.
invisible(colSums(tenx_subset))
```

In this case, `colSums()` has processed `tenx_subset` in `r length(defaultAutoGrid(tenx_subset))` blocks.
The verbose progress report tells us that these blocks are over the *rows* of `tenx_subset`.

Let's take a look at a few more examples of functions implemented with block processing.

#### Functions in DelayedArray

Some of the most useful functions in the `r BiocStyle::Biocpkg("DelayedArray")` package implement common operations on a *DelayedMatrix* using block processing.
These include the following row and column summarization methods:

- `rowSums()`
- `colSums()`
- `rowMeans()`
- `colMeans()`
- `rowMaxs()`
- `colMaxs()`
- `rowMins()`
- `colMins()`
- `rowRanges()`
- `colRanges()`

Two useful but lesser known functions use block processing to compute column/row sums of a *DelayedMatrix* based on a grouping variable:

- `rowsum()`
- `colsum()`

Matrix multiplication is also implemented using block processing:

```{r}
# This is mathematically equivalent to colSums(tenx_subset).
matrix(1, ncol = nrow(tenx_subset)) %*% tenx_subset
```

#### Functions in DelayedMatrixStats

We've already seen the `r BiocStyle::Biocpkg("DelayedMatrixStats")` package in action back when computing the [Median expression of each gene].
`r BiocStyle::Biocpkg("DelayedMatrixStats")` is a port of the `r BiocStyle::CRANpkg("matrixStats")` package's API for use with *DelayedMatrix* objects.
It provides [more than 70 functions](https://github.com/PeteHaitch/DelayedMatrixStats#api-coverage) that apply to rows and columns of *DelayedMatrix* objects.

#### Try it yourself

Try out some of the block processing functions from `r BiocStyle::Biocpkg("DelayedArray")` and `r BiocStyle::Biocpkg("DelayedMatrixStats")` on `tenx_subset`^[If you try these out on `tenx` you might be waiting a while, so I don't recommend this in the workshop.].

### General block processing

As we have seen, many common row/column summarization methods on a *DelayedMatrix* have already been implemented in `r BiocStyle::Biocpkg("DelayedArray")` and `r BiocStyle::Biocpkg("DelayedMatrixStats")`.
Nonetheless, there may be times you need to implement your own algorithm using block processing.
The documentation on this topic is a little sparse, but some details can be found in `help("block_processing", "DelayedArray"),` `help("ArrayGrid", "DelayedArray")`, `help("blockApply", "DelayedArray")`, and `help("AutoGrid", "DelayedArray")` or by reading the source code of the aforementioned packages. 
Briefly, to perform block processing requires that you:

1. Set up an *ArrayGrid* over the *DelayedArray* to be processed. This specifies the block structure that will be traversed when processing the *DelayedArray*.
    - The `defaultAutoGrid()`, `rowAutoGrid()`, and `colAutoGrid()` functions can help set up the *ArrayGrid*.
2. Iterate over the *DelayedArray* via the *ArrayGrid* to read each block of data into memory as an ordinary (i.e. dense) or sparse array and compute the statistic for that block.
    - The `blockApply()` and `blockReduce()` functions can help perform the block processing, even incorporating parallelization via the `r BiocStyle::Biocpkg("BiocParallel")` package.
3. Appropriately combine the block-level statistics to get your final result.
    - This is typically up to you as a developer of the function.

#### Example

Let's implement a basic version of `colSums()` where we define each block to be a column 

```{r}
basic_colSums <- function(x) {
  # 1. Set up the ArrayGrid.
  grid <- colAutoGrid(x, ncol = 1)
  # 2. Load the blocks into memory and compute the block-level statistics.
  block_level_stat <- blockApply(x, colSums, grid = grid)
  # 3. Combine the block-level statistics.
  unlist(block_level_stat)
}

# Check basic_colSums() gives the correct result.
identical(basic_colSums(tenx_subset), colSums(tenx_subset))
```

#### Try it yourself

- Try modifying `basic_colSums()` to define each block as a group of 100 columns.
- Try modifying `basic_colSums()` to define each block using the `defaultAutoGrid()`.

### Realization

To *realize* a *DelayedArray* object is to trigger execution of the delayed operations carried by the object and return the result as an ordinary (i.e. dense) or sparse array.

#### Realizing in-memory

We can realize a *DelayedArray* in memory as an ordinary (i.e. dense) or sparse array.

To realize a *DelayedArray* as an ordinary array, we can call `as.array()` on it.

```{r}
tenx_subset_realized <- as.array(tenx_subset)
class(tenx_subset_realized)
```

You may have noticed that the `tenx_subset` data contains a lot of zero values.
We might therefore opt to realize the data as a sparse matrix, specifically a sparse matrix from the `r BiocStyle::CRANpkg("Matrix")` package.

```{r}
tenx_subset_sparse <- as(tenx_subset, "sparseMatrix")
class(tenx_subset_sparse)
```

Realizing as a sparse matrix is particularly useful when the data are stored in a sparse *DelayedArray* object^[A sparse *DelayedArray* is one for which `is_sparse()` returns `TRUE`.], such as the *TENxMatrix* object described below.

#### Realizing to disk

[Realizing in-memory] realizes the entire object in memory, which could require too much RAM if the object is large[^12].
Therefore, a large *DelayedArray* object may preferably by realized to disk.

[^12]: In the above example it's safe because `tenx_subset` only requires around 100 Mb of memory even as an ordinary array.

Here we will demonstrate by realizing to an HDF5 file, but we could also realize to another on-disk backend such as a TileDB array (see [Realization backends]).
Realizing to an HDF5 file requires that the `r BiocStyle::Biocpkg("HDF5Array")` package is installed.

We can realize to disk as a dense array in an HDF5 file (*HDF5Array*) or as a sparse array (*TENxMatrix*).

To realize a *DelayedArray* as a dense array in an HDF5 file, we can call `writeHDF5Array()` on it.

```{r}
tenx_subset_hdf5 <- writeHDF5Array(tenx_subset)
```

To realize a *DelayedArray* as a sparse array in an HDF5 file, we can call `writeTENxMatrix()` on it.

```{r}
tenx_subset_sparse_hdf5 <- writeTENxMatrix(tenx_subset)
```

Notice that the process of realization used block processing, which avoids loading the entire dataset entire memory and that the results of these realizations are an *HDF5Matrix* and a *TENxMatrix*, respectively^[The *TENxMatrix* class follows an HDF5-based sparse matrix representation, instead of the conventional (i.e. dense) HDF5 representation, and is used by 10x Genomics.].

```{r}
class(tenx_subset_hdf5)
class(tenx_subset_sparse_hdf5)
```

Furthermore, neither `tenx_subset_hdf5` nor `tenx_subset_sparse_hdf5` carry around the delayed operations of `tenx_subset` because these have been *realized* before writing the data 

```{r}
# Compare the trees of delayed operations.
showtree(tenx_subset_hdf5)
showtree(tenx_subset_sparse_hdf5)
showtree(tenx_subset)
```

Used like this, `writeHDF5Array()` and `writeTENxMatrix()` will write their results to a file in the *HDF5 dump directory*, a dumping ground for automatically created HDF5 datasets.
We can see a log of the operations that have written to the HDF5 dump directory using `showHDF5DumpLog()`.

```{r}
showHDF5DumpLog()
```

Often, however, we will want full control of where and how the data are written to the HDF5 file[^14] and the `writeHDF5Array()` and `writeTENxMatrix()` functions give you full control over this and more.

[^14]: We'll discuss why you might want this later in the workshop.

```{r}
# Write the data to a user-specified HDF5 file using maximum compression and 
# 'chunking' along the columns.
my_hdf5_file <- tempfile(fileext = ".h5")
tenx_subset_my_file_hdf5 <- writeHDF5Array(
  tenx_subset,
  filepath = my_hdf5_file,
  chunkdim = c(nrow(tenx_subset), 1),
  level = 9)

# Compare `tenx_subset_hdf5` to `tenx_subset_my_file_hdf5`
seed(tenx_subset_hdf5)
seed(tenx_subset_my_file_hdf5)
```

#### Realization backends

We've now seen that we can realize to an HDF5 file.
This is called the HDF5Array 'realization backend' and is implemented in the `r BiocStyle::Biocpkg("HDF5Array")` package.
There are a few other realization backends to be aware of.

```{r}
supportedRealizationBackends()
```

The `r BiocStyle::Githubpkg("LTLA/TileDBArray")` provides a TileDB realization backend, but this packages is not yet available from Bioconductor.

There is also the `NULL` backend, which means the data are realized in memory as an ordinary *array* and then wrapped in a *DelayedArray*.
This is the default realization backend upon loading/attaching the `r BiocStyle::Biocpkg("DelayedArray")` package.

```{r}
getRealizationBackend()
```

The default realization backend can be altered with `setRealizationBackend()`.

```{r}
setRealizationBackend("HDF5Array")
getRealizationBackend()
setRealizationBackend(NULL)
getRealizationBackend()
```

It can be important to know what your current realization backend is because it will be used implicitly by some functions.
For example, matrix multiplication that involves a *DelayedMatrix* uses the current realization backend.

```{r}
setRealizationBackend("HDF5Array")
tenx_subset %*% matrix(1, nrow = ncol(tenx_subset))

setRealizationBackend(NULL)
tenx_subset %*% matrix(1, nrow = ncol(tenx_subset))
```

#### The `realize()` function

We've seen that we can realize to the HDF5Array backend using `writeHDF5Array(tenx_subset)`.
We could also use coercion by calling `as(tenx_subset, "HDF5Array")`.
A third way of realizing a *DelayedArray* to an HDF5 file is with the `realize()` function.

```{r}
realize(tenx_subset, BACKEND = "HDF5Array")
```

So why might you use `realize()` instead of these other options?
Because it allows us to easily switch out the realization backend and will defer to the current realization backend if none is supplied.

```{r}
# Realize to the current realization backend.
getRealizationBackend()
realize(tenx_subset)

# # Realize as an RleArray.
setRealizationBackend("RleArray")
tenx_subset_rlearray <- realize(tenx_subset)
tenx_subset_rlearray

# Switch back to the default backend.
setRealizationBackend(NULL)
```

This is probably most useful when writing package code so that you can allow the user control over the realization backend.

## Package ecosystem

The DelayedArray framework is, unsurprisingly, implemented in the `r BiocStyle::Biocpkg("DelayedArray")` package.
However, there are several other key packages that are an important part of the broader 'ecosystem'.
More importantly, as a user of Bioconductor software, it is increasingly likely that you will encounter *DelayedArray* objects during a data analysis, especially if you are analysing single-cell data[^15].
The following table lists packages that depend upon the `r BiocStyle::Biocpkg("DelayedArray")` package.

[^15]: In fact, if you use any package that makes use of the *SummarizedExperiment* class, then you will almost certainly load the `r BiocStyle::Biocpkg("DelayedArray")` package during the course of your analysis, whether you know it or not! This is because `r BiocStyle::Biocpkg("SummarizedExperiment")` depends upon `r BiocStyle::Biocpkg("DelayedArray")`.

```{r}
dep_tbl <- BiocPkgTools::buildPkgDependencyDataFrame()
da_dep_tbl <- dep_tbl[dep_tbl$dependency == "DelayedArray", 
                      c("Package", "edgetype")]
da_dep_tbl <- da_dep_tbl[with(da_dep_tbl, order(edgetype, Package)), ]
colnames(da_dep_tbl) <- c("Package", "Dependency Type")
rmarkdown::paged_table(da_dep_tbl)
```

We will briefly highlight some of the key packages in this table, broadly categorising these as 'user-focused'/'user-facing' or 'developer-focused' packages and those that span the spectrum.

### Packages that both users and developers should probably know about

#### `r BiocStyle::Biocpkg("DelayedArray")`

Well, duh.
Implements the *DelayedArray* and *RleArray* classes, along with all the fundamentals the enable the delayed operations, block processing, and realization that underpin the DelayedArray framework.

#### `r BiocStyle::Biocpkg("HDF5Array")`

Implements the *HDF5Array* and *TENxMatrix* classes, two convenient and memory-efficient array-like containers for on-disk representation of HDF5 datasets. 
*HDF5Array* is for datasets that use the conventional (i.e. dense) HDF5 representation.
*TENxMatrix* is for datasets that use the HDF5-based sparse matrix representation from 10x Genomics.

#### `r BiocStyle::Githubpkg("LTLA/TileDBArray")`

Implements a DelayedArray backend for [TileDB](https://tiledb.com/) to read, write and store dense and sparse arrays.
The resulting *TileDBArray* objects are directly compatible with any Bioconductor package that accepts *DelayedArray* objects, serving as a swap-in replacement for the predominant *HDF5Array* that is currently used throughout the Bioconductor ecosystem for representing large datasets.

**NB**: `r BiocStyle::Githubpkg("LTLA/TileDBArray")` is not yet available from Bioconductor.

#### `r BiocStyle::Biocpkg("DelayedMatrixStats")`

A port of the `r BiocStyle::CRANpkg("matrixStats")` API for use with *DelayedMatrix* objects.
High-performing functions operating on rows and columns of *DelayedMatrix* objects, e.g. `col` / `rowMedians()`, `col` / `rowRanks()`, and `col` / `rowSds()`.
Functions optimized per data type and for subsetted calculations such that both memory usage and processing time is minimized. 

Disclaimer: I wrote this one.

#### `r BiocStyle::Biocpkg("BiocSingular")`

Implements exact and approximate methods for singular value decomposition and principal components analysis using a framework that allows them to be easily switched within Bioconductor packages or workflows.
These methods work on *DelayedMatrix* objects as well as ordinary *matrix* objects and some sparse matrix objects from the `r BiocStyle::CRANpkg("Matrix")` package.

`r BiocStyle::Biocpkg("BiocSingular")` defines a few interesting specialized *DelayedMatrix* subclasses that aim to preserve sparsity of the original matrix:

- *DeferredMatrix*: Supports deferred centering and scaling of the columns of a matrix prior to principal components analysis.
- *LowRankMatrix*: Provides a memory-efficient representation of a low-rank reconstruction, e.g., after a principal components analysis.
- *ResidualMatrix*: Supports delayed calculation of the residuals from a linear model fit, usually prior to principal components analysis.

#### `r BiocStyle::Biocpkg("VCFArray")` and `r BiocStyle::Biocpkg("GDSArray")`

Implements the *VCFArray* and *GDSArray* classes, types of *DelayedArray*, to represent VCF files and GDS-files in an array-like representation. VCF and GDS files are widely used to represent genotyping or sequence data.

#### `r BiocStyle::Biocpkg("rhdf5client")` and `r BiocStyle::Biocpkg("restfulSE")`

Provide functions and classes to interface with remote data stores by operating on *SummarizedExperiment*-like objects.
These data are HDF5 files living on a remote server running `h5serv`, a REST-based service for HDF5 data.

### User-focused/user-facing packages

These are the packages that as a user you might directly load/attach with `library()` as part of a data analysis.
Alternatively, these may be loaded/attached as a dependency[^16] of another package you load/attach as part of an analysis.

[^16]: As listed in the `Depends` field of the package `DESCRIPTION` file.

#### `r BiocStyle::Biocpkg("DropletUtils")`

Provides a number of utility functions for handling single-cell (RNA-seq) data from droplet technologies such as 10X Genomics.
This includes `read10xCounts()` for data loading from the count matrices produced by 10x Genomics' **CellRanger** software, which may be stored in an HDF5 file. 
To do this, it makes use of the *TENxMatrix* class.

#### `r BiocStyle::Biocpkg("LoomExperiment")`

Provides a means to convert from 'loom' files to standard Bioconductor classes and back again.
The [Loom file format](http://linnarssonlab.org/loompy/index.html) uses HDF5 to store experimental data and is used by some tools and labs producing data using single-cell assays.
This includes the `import()` function for data loading from loom files into an *HDF5Matrix*.

#### `r BiocStyle::Biocpkg("scuttle")`

Provides basic utility functions for performing single-cell analyses, focusing on simple normalization, quality control and data transformations.
Also provides some helper functions to assist development of other packages.
These methods work on *DelayedMatrix* objects as well as ordinary *matrix* objects and some sparse matrix objects from the `r BiocStyle::CRANpkg("Matrix")` package.

#### `r BiocStyle::Biocpkg("batchelor")`

Implements a variety of methods for batch correction of single-cell (RNA sequencing) data, such as `multiBatchPCA()` and `fastMNN()`.
These methods work on *DelayedMatrix* objects as well as ordinary *matrix* objects and some sparse matrix objects from the `r BiocStyle::CRANpkg("Matrix")` package.

#### `r BiocStyle::Biocpkg("bsseq")`

A collection of tools for analyzing and visualizing bisulfite sequencing data.
This was one of the first packages to make use of the DelayedArray framework and it supports these throughout the package.
This was needed in order to store and analyse large non-CpG methylation datasets (\> 300 million loci, hundreds of samples) using HDF5 files.

Disclaimer: I did this re-write of `r BiocStyle::Biocpkg("bsseq")` and learnt a lot along the way.

#### `r BiocStyle::Biocpkg("minfi")`

Tools to analyze & visualize Illumina Infinium methylation arrays.
This doesn't have the same level of support for *DelayedMatrix* objects as `r BiocStyle::Biocpkg("bsseq")`, but perhaps one day.
This is needed in order to store and analyse large methylation datasets (\> 850,000 loci, tens of thousands of) using HDF5 files.

Disclaimer: This was the second package, after `r BiocStyle::Biocpkg("bsseq")`, I started to re-write to support the DelayedArray framework. 
Here, it is rather more difficult because it is a 'widely' used package and has code from lots of different authors with different styles.

### Developer-focused packages

#### `r BiocStyle::Biocpkg("beachmat")`

Provides a consistent C++ class interface for reading from and writing data to a variety of commonly used matrix types.
Ordinary matrices and several sparse/dense `r BiocStyle::CRANpkg("Matrix")` classes are directly supported, third-party S4 classes may be supported by external linkage (such as the *HDF5Matrix* class), while all other matrices are handled by DelayedArray block processing.

## Workflow tips for DelayedArray-backed analyses

We'll conclude with some miscellaneous tips I've collected over the past few years of using DelayedArray-backed workflows.

To demonstrate, we'll create a *SingleCellExperiment* object containing `tenx_subset` (a subset of the `tenx` counts data).

```{r}
library(SingleCellExperiment)
sce <- SingleCellExperiment(assays = list(counts = tenx_subset))
```

We term this an HDF5-backed SummarizedExperiment because:

1. A *SingleCellExperiment* is (a derivative of) a *SummarizedExperiment*.
2. The assay data are stored in an HDF5 file.

```{r}
sce
# We'll discuss the use of `withDimnames = FALSE` shortly.
assay(sce, withDimnames = FALSE)
```

To make the example a little bit more interesting, we'll also normalize the data.

```{r}
library(scuttle)
sce <- computeLibraryFactors(sce)
sce <- logNormCounts(sce)
```

The resulting *SingleCellExperiment* object contains two assays - `counts` and `logcounts` - both of which are *DelayedMatrix* objects.

```{r}
assays(sce)

assay(sce, "counts", withDimnames = FALSE)
assay(sce, "logcounts", withDimnames = FALSE)
```

### Saving and loading HDF5-backed SummarizedExperiment objects

#### Short version

Use `saveHDF5SummarizedExperiment()`, `quickResaveHDF5SummarizedExperiment()`, and `loadHDF5SummarizedExperiment()` rather than `saveRDS()` and `readRDS()` or `save()` and `load()` when saving/loading HDF5-backed *SummarizedExperiment* objects.

Here is an example:

```{r}
# Specify the directory where you want to save the object.
# Here we use a temporary directory.
dir <- file.path(tempdir(), "my_h5_se")
saveHDF5SummarizedExperiment(sce, dir, verbose = TRUE)

# Load the saved object.
saved_sce <- loadHDF5SummarizedExperiment(dir)
```

Note that this directory is relocatable i.e. it can be moved (or copied) to a different place, on the same or a different computer, before calling `loadHDF5SummarizedExperiment()` on it.
For convenient sharing with collaborators, it is suggested to turn it into a tarball (with Unix command tar), or zip file, before the transfer^[Please keep in mind that `saveHDF5SummarizedExperiment()` and `loadHDF5SummarizedExperiment()` don't know how to produce/read tarballs or zip files at the moment, so the process of packaging/extracting the tarball or zip file is entirely the user responsibility. This is typically done from outside R.].

Calling `saveHDF5SummarizedExperiment()` will realize any delayed operations prior to saving the assay data in an HDF5 file, as illustrated below.

```{r}
# Compare the trees of delayed operations.
showtree(logcounts(sce, withDimnames = FALSE))
showtree(logcounts(saved_sce, withDimnames = FALSE))
```

Finally please note that, depending on the size of the data to write to disk and the performance of the disk, `saveHDF5SummarizedExperiment()` can take a long time to complete^[Use `verbose=TRUE` to see its progress.].

The `quickResaveHDF5SummarizedExperiment()` function can be useful if you have updated a *SummarizedExperiment* already created by an earlier call to `saveHDF5SummarizedExperiment()`.
For example, suppose you created an HDF5-backed *SummarizedExperiment*, do some pre-processing of the data, and want to save the result.

Here is the object before the pre-processing:

```{r}
saved_sce
```

Now, let's mock up pre-processing by adding sample metadata to the *colData*, and excluding certain features (rows) and samples (columns).

```{r}
# Mock adding sample metadata.
saved_sce$sample <- c(rep("S1", 400), rep("S2", 600))

# Mock excluding certain features and samples.
keep_feature <- rbinom(nrow(sce), 1, 0.95)
keep_sample <- rbinom(ncol(sce), 1, 0.8)
saved_sce <- saved_sce[keep_feature, keep_sample]
```

This is the object after the pre-processing:

```{r}
saved_sce
```

We can use `quickResaveHDF5SummarizedExperiment()` quickly re-save the pre-processed object.
This is generally much faster than the initial call to `saveHDF5SummarizedExperiment()` because it does not re-write the assay data to an HDF5 file.

```{r}
quickResaveHDF5SummarizedExperiment(saved_sce)
```

Notice that the pre-processing is preserved when we re-load the re-saved HDF5-backed *SummarizedExperiment*.

```{r}
loadHDF5SummarizedExperiment(dir)
```

#### Long version

A HDF5-backed *SummarizedExperiment* is a light-weight shell (the *SummarizedExperiment*) around a large disk-backed data matrix (the *HDF5Matrix*).
The following explanation comes from `help("saveHDF5SummarizedExperiment", "HDF5Array")`:

Roughly speaking, `saveRDS()` only serializes the part of an object that resides in memory[^17].
For most objects in R, that's the whole object, so `saveRDS()` does the job.

[^17]: The reality is a little bit more nuanced, but discussing the full details is not important here, and would only distract us

However some objects are pointing to on-disk data.
For example:

- A *TxDb* object from the `r BiocStyle::Biocpkg("GenomicFeatures")` points to an SQLite database
- An *HDF5Array* object points to a dataset in an HDF5 file
- A *SummarizedExperiment* derivative can have one or more of its assays that point to datasets (one per assay) in an HDF5 file.

These objects have 2 parts: one part is in memory, and one part is on disk.
The 1st part is sometimes called the object shell and is generally thin (i.e. it has a small memory footprint).
The 2nd part is the data and is typically big. 
The object shell and data are linked together via some kind of pointer stored in the shell (e.g. an SQLite connection, or a path to a file, etc.).
Note that this is a one way link in the sense that the object shell 'knows' where to find the on-disk data but the on-disk data knows nothing about the object shell (and is completely agnostic about what kind of object shell could be pointing to it).
Furthermore, at any given time on a given system, there could be more than one object shell pointing to the same on-disk data.
These object shells could exist in the same R session or in sessions in other languages (e.g. Python).
These various sessions could be run by the same or by different users.

Using `saveRDS()` on such object will only serialize the shell part so will produce a small `.rds` file that contains the serialized object shell but not the object data.

This is problematic because:

1.  If you later unserialize the object (with `readRDS()`) on the same system where you originally serialized it, it is possible that you will get back an object that is fully functional and semantically equivalent to the original object. But here is the catch: this will be the case **ONLY** if the data is still at the original location and has not been modified (i.e. nobody wrote or altered the data in the SQLite database or HDF5 file in the mean time), and if the serialization/unserialization cycle didn't break the link between the object shell and the data (this serialization/unserialization cycle is known to break open SQLite connections).
2.  After serialization the object shell and data are stored in separate files (in the new `.rds` file for the shell, still in the original SQLite or HDF5 file for the data), typically in very different places on the file system. But these 2 files are not relocatable, that is, moving or copying them to another system or sending them to collaborators will typically break the link between them. Concretely this means that the object obtained by using `readRDS()` on the destination system will be broken.

`saveHDF5SummarizedExperiment()` addresses these issues by saving the object shell and assay data in a folder that is relocatable.

Note that it only works on *SummarizedExperiment* derivatives.
What it does exactly is:

1.  Write all the assay data to an HDF5 file
2.  Serialize the object shell, which in this case is everything in the object that is not the assay data.

The 2 files (HDF5 and `.rds`) are written to the directory specified by the user.
The resulting directory contains a full representation of the object and is relocatable, that is, it can be moved or copied to another place on the system, or to another system (possibly after making a tarball of it), where `loadHDF5SummarizedExperiment()` can then be used to load the object back in R.

`quickResaveHDF5SummarizedExperiment()` preserves the HDF5 file and datasets that the assays in the *SummarizedExperiment* are already pointing to (and which were created by an earlier call to `saveHDF5SummarizedExperiment()`).
All it does is re-serialize the *SummarizedExperiment* on top of the `.rds` file that is associated with this HDF5 file (and which was created by an earlier call to `saveHDF5SummarizedExperiment(` or `quickResaveHDF5SummarizedExperiment()`).
Because the delayed operations possibly carried by the assays in x are not realized, this is very fast.

### Block geometry

The block geometry (size and shape) are key determinants of performance when designing/applying functions to *DelayedArray* objects.
For example, functions may be faster if fewer blocks are required (e.g., to minimise reading data from disk) or a function may require a sample's complete data (i.e. a full column of data) to work at all.

A function should enforce a 'minimal' block geometry (i.e. the geometry required to produce a correct result) whilst also allowing the user to alter the block geometry for improved performance on their system and, ideally, offering some sort of automatic block geometry that is reasonably performant in most applications of the function.

The `r BiocStyle::Biocpkg("DelayedArray")` sets some default values that control the geometry of the automatic blocks.
These are the automatic block size and the block shape, of which the block size is more relevant to the user.

#### Block size

The `getAutoBlockSize()` gives the automatic size in bytes of a block used when performing automatic block processing.
By default, this is set to `r getAutoBlockSize()` meaning each block can use up to `r getAutoBlockSize() / 1e6` Mb of data.

Using fewer, larger blocks generally means faster performance (at the cost of higher peak memory usage).
Conversely, using more, smaller blocks generally means slower performance (at the benefit of lower peak memory usage).
Therefore, a user may wish to increase/decrease this on machines with sufficient memory by using `setAutoBlockSize()` or by specifying the block size in functions that accept this as an argument.

```{r}
system.time(colSums(counts(sce, withDimnames = FALSE)))

# Increasing the block size 10-fold.
setAutoBlockSize(1e9)
system.time(colSums(counts(sce, withDimnames = FALSE)))

# Decreasing the default block size 100-fold
setAutoBlockSize(1e6)
system.time(colSums(counts(sce, withDimnames = FALSE)))

# Reverting to default block size.
setAutoBlockSize(1e8)
```

#### Block shape

The choice of block shape is typically be more relevant to the function's designer rather than its users.
It is desirable to have functions that can handle arbitrary block shapes.
However, this is not always possible, such as a function that requires that the data be processed by column.

### Chunking

Data stored on disk, such as in an HDF5 file, are usually 'chunked' into sub-matrices (for matrix data) or hyper-cubes (for arrays of higher dimension) to allow for more efficient subset selection.
For example, we could choose to chunk an $R \times C$ matrix by column, by row, or into $r \times c$ sub-matrices ($r \leq R, c \leq C$).

We'll illustrate chunking with the HDF5 format, but similar concepts exist for other disk-backed data (e.g., *TileDBArray*) and even to in-memory data (e.g., *RleArray*).

#### Chunk geometry

With the `r BiocStyle::Biocpkg("HDF5Array")` package, chunking can be controlled by the `chunkdim` argument when writing data to disk using the `writeHDF5Array()` or `saveHDF5SummarizedExperiment()` functions.

In general, you want your data to be chunked in a manner that supports the type of access patterns you will be subsequently making during your analysis.
For example, if you know you only need to access data by column then chunk the data by column.
Of course, you often either don't know in advance what access patterns you need or you need both row and column access.
In that case, chunking into sub-matrix/hyper-cubes offers the best tradeoff.

We'll demonstrate how chunking can affect performance by computing the column sums of column-chunked and row-chunked versions of the same matrix.
The row-chunked data are much slower to process than the column-chunked data because it requires many more reads from disk.

```{r}
# Simulate some data.
x <- matrix(sample(1e8), ncol = 1e2, nrow = 1e6)

# Turn of verbose progress reporting.
DelayedArray:::set_verbose_block_processing(FALSE)

# Create column-chunked and row-chunked HDF5Matrix objects.
x_col <- writeHDF5Array(x, chunkdim = c(nrow(x), 1))
x_row <- writeHDF5Array(x, chunkdim = c(1, ncol(x)))

# Time computing the column sums.
system.time(colSums(x_col))
system.time(colSums(x_row))
```

#### Chunk compression

With the `r BiocStyle::Biocpkg("HDF5Array")` package, the chunks can be compressed before writing to disk.
Greater compression will lead to smaller files on disk but will generally require longer to write the files and may require longer to process the files.
The level of compression is controlled by the by the `level` argument when writing data to disk using the `writeHDF5Array()` or `saveHDF5SummarizedExperiment()` functions.

We'll demonstrate how compression can affect performance by writing uncompressed and maximally-compressed versions of the same matrix to disk and then computing the column sums.
The uncompressed data are much faster to write but it takes roughly the same time to compute the column sums on both the uncompressed and maximally compressed data.
These simulated data are not very compressible^[E.g., it's uniformly distributed with few zeros], but the on-disk savings will be more dramatic for other arrays (e.g., try repeating this with the `tenx_subset` data).

```{r}
# Simulate some data.
x <- matrix(sample(1e8), ncol = 1e2, nrow = 1e6)

# Time creating uncompressed and maximally-compressed HDF5Matrix objects.
system.time(x_no_compression <- writeHDF5Array(x, level = 0))
system.time(x_max_compression <- writeHDF5Array(x, level = 9))

# Size of uncompressed and maximally-compressed HDF5 files on disk.
file.size(path(x_no_compression))
file.size(path(x_max_compression))

# Time computing the column sums.
system.time(colSums(x_no_compression))
system.time(colSums(x_max_compression))
```

### Interaction of block geometry and chunk geometry

Within the *DelayedArray* framework, the [Block geometry] and [Chunk geometry] are similar but distinct concepts.
The difference is this:

> **The block geometry dictates how the data are accessed, the chunk geometry dictates how the data are stored**.

As noted in [Chunk geometry], when you don't know in advance what access patterns you need or you need both row and column access, then chunking into sub-matrix/hyper-cubes is often the best tradeoff.

As a user, increasing the [Block size] is often the easiest way to achieve faster block processing (at the cost of higher peak memory usage) but it may sometimes be beneficial to re-save your data with a chunk geometry that better matches the block geometry of the function(s) you are calling.

### Avoid subsetting too much and random access patterns

Reordering/subsetting the data may degrade the performance of even seemingly simple operations.
This is especially true of disk-backed data, where performance is best when reading contiguous chunks of data and worst when having to read data with a random access pattern.
This is demonstrated below by computing the column sums of the `tenx_subset` and a row-scrambled version thereof.

```{r}
# Compute column sums of tenx_subset.
system.time(colSums(tenx_subset))

# Compute column sums of a row-scrambled version of tenx_subset.
y <- tenx_subset[sample(nrow(tenx_subset)), ]
system.time(colSums(y))
```

### Process, save, repeat

When analysing large datasets, a workflow that is broken up into stages and saves the intermediate outputs can be help preserve one's sanity.
This is true regardless of whether the DelayedArray framework is used - it sucks having to repeat some long pre-processing computation in order to make a quick plot - but it is especially true for DelayedArray-backed analyses where the accumulation of delayed operations may eventually lead to degraded performance.

This means using `saveHDF5SummarizedExperiment()`/`quickResaveHDF5SummarizedExperiment()` and `loadHDF5SummarizedExperiment()` following time consuming processing of the object.

### Pragmatism rules

The DelayedArray framework coupled with disk-based data can be a powerful way to keep memory usage down, but sometimes you need to apply an algorithm that is too slow or simply doesn't work except on ordinary in-memory arrays.
Pragmatism is required: find a machine with a lot of RAM, load the data into memory, compute the thing you need, and move on with your life.

For example, with whole-genome bisulfite sequencing (WGBS) data there are 3 very large matrices stored in the *SummarizedExperiment* object.
But to make a plot of methylation values along a gene promoter, a common requirement, you only need to load in a small 'slice' of one of these matrices.
With a HDF5-backed *SummarizedExperiment* object you can quickly do this.

This brings us to a perhaps under-appreciated advantage of using HDF5-backed *SummarizedExperiment* objects: loading the saved data with `HDF5Array::loadHDF5SummarizedExperiment()` is often **much** faster than loading the in-memory equivalent *SummarizedExperiment*  with `readRDS()`.
I made extensive use of this when processing large WGBS datasets as I could quickly load the *SummarizedExperiment* object to compute summaries of the sample metadata (stored in the `colData` of the *SummarizedExperiment* object), a process that used to take tens of minutes to hours because the 3 large matrices also had to be loaded into memory.
This has been so useful to me that I now keep even 'small' WGBS datasets as HDF5-backed *SummarizedExperiment* objects.

### Avoid 'degrading' to a *DelayedArray*

The DelayedArray framework is implemented using the S4 object oriented system.
This can be used to write methods that are optimized for a particular backend. For example, we might write a `colMaxs()` method that is optimized for the *TENxMatrix* class by exploiting the sparse storage mode of the underlying data.
In order for our hypothetical `colMaxs()` to 'know' that it can use this optimized method, however, it would need the data to be supplied as a *TENxMatrix* instance.

Unfortunately, it is very easy to 'degrade' a specialised *DelayedArray* derivative to a *DelayedArray*, as we will demonstrate.
Let's start with a *TENxMatrix* object:

```{r}
class(tenx_subset_sparse_hdf5)
```

Some common operations degrade the result to a *DelayedMatrix*, for example:

```{r}
# Subsetting.
class(tenx_subset_sparse_hdf5[, 1:10])

# Setting dimnames
val <- tenx_subset_sparse_hdf5
dimnames(val) <- list(paste0("R", seq_len(nrow(val))), NULL)
class(val)
```

A common scenario where this degrading may occur is when extracting the data from a *SummarizedExperiment*.

```{r}
# Construct a SummarizedExperiment with a TENxMatrix as an assay.
se <- SummarizedExperiment(
  list(counts = tenx_subset_sparse_hdf5))
# Add some column names.
colnames(se) <- paste0("S", seq_len(ncol(se)))

# Check the class of the assay.
class(assay(tmp))
```

What's happened here?
By default, `assay(se)` calls `assay(se, withDimnames = TRUE)` which has the effect of copying the dimnames from the *SummarizedExperiment* and adding them to the returned assay data.
As we saw above, setting the dimnames on a *TENxMatrix* (or other *DelayedMatrix* derivative) will degrade it to a *DelayedMatrix*.
Consequently, running `colMaxs(assay(se))` will not call our (hypothetical) optimized method for *TENxMatrix* objects and will instead defer to the slower, more general block processing method that is implemented for *DelayedMatrix* objects.

To avoid this 'degrading upon assay extraction', we can should set `withDimnames = FALSE`.

```{r}
class(assay(se, withDimnames = FALSE))
```

More generally, you may need to avoid degrading a *DelayedArray* derivative to a *DelayedArray* in order to use backend-optimized methods.

### Make use of sparsity

Some recent additions to the *DelayedArray* framework allow it to better preserve the sparsity of the data.
For example, we can preserve the sparsity of $log(x + 1)$ when applied to the sparse `tenx_subset_sparse_hdf5` object.

```{r}
is_sparse(tenx_subset_sparse_hdf5) # TRUE
is_sparse(tenx_subset_sparse_hdf5 + 1) # FALSE
is_sparse(log(tenx_subset_sparse_hdf5 + 1)) # TRUE
```

### Parallelization strategies

Several of the routines we used in today's workshop have 'native' parallelization via the `r BiocStyle::Biocpkg("BiocParallel")` package.
Look out for the `BPPARAM` argument in function documentation[^20].

[^20]: Unfortunately, simply typing the function name at the command prompt or using `args()` may not be sufficient to identify these functions. E.g., `args(denoisePCA)` doesn't include `BPPARAM` but `help("denoisePCA", "scran")` tells you this is an option (Footnote to the footnote: this is because `denoicePCA()` is an S4 generic with methods defined only for the first argument, `x`. The methods include the `BPPARAM` argument and are documented as such.)

Fair warning: parallelization is tricky and performance may not match expectations.

### What's in my HDF5 file?

I do this quite a bit: I've got an HDF5 file but I don't know (or have forgotten) what's in it. `rhdf5::h5ls()` to the rescue.

```{r}
rhdf5::h5ls(path(counts(sce)))
```

## Important topics I didn't discuss

This 1-hour workshop can not cover everything about the DelayedArray framework.
Some important topics that I omitted and some brief references to further resources are:

- General construction of a disk-backed *DelayedArray* (e.g., *HDF5Array*) from a bunch of files (e.g., CSV files, BAM files, BED files, BigWig files, etc.)
   - This requires the construction of a `RealizationSink()`.
   - `bsseq::read.bismark()` and `minfi:::read.metharray2()` are (early) example of constructing a HDF5-backed *SummarizedExperiment* from a bunch of files.
- Parallelization with disk-backed data.
   - Parallelization performance depends heavily on (A) the choice of backend, and (B) your computer's hardware.
   - Rule of thumb:
       - It's never as straightforward or as big an improvement as you think/hope.
          - Parallel *writing* from files (e.g. HDF5 files) is a no go.
          - Parallel *reading* is sometimes, maybe, perhaps, okay ... 
- Choice of disk-backed backends.
    - `r BiocStyle::Biocpkg("HDF5Array")` is the *de facto* standard in Bioconductor (and HDF5 files are the *de facto* standard for disk-backed array-like datasets in scientific computing).
    - `r BiocStyle::Githubpkg("TileDB-Inc/TileDB-R")` and `r BiocStyle::Githubpkg("LTLA/TileDBArray")` look very interesting but are not available from CRAN/Bioconductor at the time of writing.

## Concluding remarks

We've now spent nearly 1 hour learning about the DelayedArray framework.
Now comes the kicker:

> **Don't use a *DelayedArray* if you can avoid it!**

I write that as someone who has a professional love for the DelayedArray framework.
I've spent many, many hours developing software that supports and extends it, leveraging it to analyse large WGBS and scRNA-seq datasets, and writing and presenting talks and workshops to teach it. 
But the simple fact remains:

> **If you can load your data into memory and still compute on it, then you're always going to have a better time doing it that way.**

Analyses will be faster, simpler, and you will have more software options available to you.
But when this isn't an option then the DelayedArray framework is a powerful set of packages to help you get your work done.
For example, I find it pretty remarkable that a first-class single-cell analysis workflow can so seamlessly support the use of both in-memory and disk-backed data.

A huge amount of gratitude is owed to Hervé Pagès, the developer of the `r BiocStyle::Biocpkg("DelayedArray")` and `r BiocStyle::Biocpkg("HDF5Array")`  packages and the broader framework; Aaron Lun, the main developer many of the key packages that build upon the DelayedArray framework (particularly for analysing scRNA-seq data); and Mike Smith, the maintainer of the `r BiocStyle::Biocpkg("rhdf5")` package.
Thank you!

## Session info

This document was prepared using the following software:

```{r}
sessionInfo()
```

## References
